{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to find the answer for T2 by using the K Nearest Neighbors algorithm. \n",
    "With this algorithm we will find top K papers which are most similar (the nearest neighbors) of a particular paper to cite. \n",
    "Afterwards, we plan to retrieve the authors of these top K papers, and find top K most frequent authors from this auhtor list.\n",
    "\n",
    "For the feature computation, we plan to use the following relevant columns:\n",
    "- Paper affiliations\n",
    "- Author research interests\n",
    "- Paper Year\n",
    "\n",
    "The process:\n",
    "1. Index all the of the categorical columns which we are going to use for our feature computation (columns: paper affiliations, author research interests)\n",
    "2. Apply One Hot encoding to these columns\n",
    "3. Create VectorAssembler for all of the accounted features\n",
    "4. Afterwards, create the pipeline for data transformation (p.1-2) and vector assembling\n",
    "5. Apply this pipeline to our dataset\n",
    "6. Divide the data into training and test set by the years\n",
    "7. Train the data using KNN algorithm from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import createDFFromFileAndSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('read data through spark').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframes from cleaned data from T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path: ./assets/cleanedDFsData/research_interests_ds.csv, schema path: ./cleanedDFsSchemas/research_interests.csv\n",
      "Types from schema: [('author_id', 'Integer'), ('research_interest', 'String')]\n",
      "File path: ./assets/cleanedDFsData/papers_ds.csv, schema path: ./cleanedDFsSchemas/paper.csv\n",
      "Types from schema: [('paper_id', 'Integer'), ('title', 'String'), ('year', 'Integer')]\n",
      "File path: ./assets/cleanedDFsData/paper_author_ds.csv, schema path: ./cleanedDFsSchemas/paper_authors.csv\n",
      "Types from schema: [('name', 'String'), ('paper_id', 'Integer'), ('author_id', 'Integer'), ('citation_count', 'Integer'), ('h_index', 'Integer'), ('paper_count', 'Integer')]\n",
      "File path: ./assets/cleanedDFsData/affiliations_ds.csv, schema path: ./cleanedDFsSchemas/affiliation.csv\n",
      "Types from schema: [('paper_id', 'Integer'), ('affiliation', 'String')]\n",
      "File path: ./assets/cleanedDFsData/publication_venues_ds.csv, schema path: ./cleanedDFsSchemas/publication_venues.csv\n",
      "Types from schema: [('paper_id', 'Integer'), ('publication_venue', 'String')]\n"
     ]
    }
   ],
   "source": [
    "### There is a new Schemas folder which contains schemas for the cleaned csvs which were \n",
    "### created at the end of load_csv_into_schema.ipynb\n",
    "CLEAN_DATA_FOLDER = './assets/cleanedDFsData/'\n",
    "SCHEMAS_FOLDER = './cleanedDFsSchemas/'\n",
    "\n",
    "### We load the neccessary csvs for T2 into the new schemas\n",
    "paper_author_df = createDFFromFileAndSchema(spark, f'{CLEAN_DATA_FOLDER}paper_author_ds.csv', f'{SCHEMAS_FOLDER}paper_authors.csv')\n",
    "paper_df = createDFFromFileAndSchema(spark, f'{CLEAN_DATA_FOLDER}papers_ds.csv', f'{SCHEMAS_FOLDER}paper.csv')\n",
    "affiliation_df = createDFFromFileAndSchema(spark, f'{CLEAN_DATA_FOLDER}affiliations_ds.csv', f'{SCHEMAS_FOLDER}affiliation.csv')\n",
    "publication_venue_df = createDFFromFileAndSchema(spark, f'{CLEAN_DATA_FOLDER}publication_venues_ds.csv', f'{SCHEMAS_FOLDER}publication_venues.csv')\n",
    "research_interests_df = createDFFromFileAndSchema(spark, f'{CLEAN_DATA_FOLDER}research_interests_ds.csv', f'{SCHEMAS_FOLDER}research_interests.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Join paper_autho_df with paper_df to retrieve information (year)\n",
    "df_for_ml_with_papers = paper_author_df.join(paper_df, 'paper_id', 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_ml_with_affiliations = df_for_ml_with_papers.join(affiliation_df, 'paper_id', 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_for_ml = df_for_ml_with_affiliations.join(research_interests_df, 'author_id', 'left').\\\n",
    "    drop('name', 'h_index', 'paper_count', 'citation_count', 'title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create String Indexers and One Hot Encoders for the research interests String columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StringIndexer\n",
    "research_interests_indexer = StringIndexer(inputCol=\"research_interest\", outputCol=\"research_interest_index\")\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "research_interests_encoder = OneHotEncoder(inputCol=\"research_interest_index\", outputCol=\"research_interest_fact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create String Indexers and One Hot Encoders for the affiliation String columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create a StringIndexer\n",
    "affiliations_indexer = StringIndexer(inputCol=\"affiliation\", outputCol=\"affiliation_index\")\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "affiliations_encoder = OneHotEncoder(inputCol=\"affiliation_index\", outputCol=\"affiliation_fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler = VectorAssembler( \\\n",
    "    inputCols=[\"year\", \"affiliation_fact\", \"research_interest_fact\"], \\\n",
    "    outputCol=\"features\" \\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_pipe = Pipeline(\n",
    "    stages=[research_interests_indexer,\\\n",
    "                research_interests_encoder, \\\n",
    "                affiliations_indexer, \\\n",
    "                affiliations_encoder, \\\n",
    "                vec_assembler \\\n",
    "           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/21 17:19:49 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from t-193.vc-graz.ac.at:63140 in 10000 milliseconds\n",
      "22/01/21 17:20:06 WARN Executor: Issue communicating with driver in heartbeater]\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 12 more\n",
      "22/01/21 17:25:45 ERROR Utils: Uncaught exception in thread driver-heartbeater9]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.HashMap.resize(HashMap.java:699)\n",
      "\tat java.base/java.util.HashMap.putVal(HashMap.java:624)\n",
      "\tat java.base/java.util.HashMap.put(HashMap.java:607)\n",
      "\tat java.management/com.sun.jmx.mbeanserver.MXBeanProxy$Visitor.visitAttribute(MXBeanProxy.java:71)\n",
      "\tat java.management/com.sun.jmx.mbeanserver.MXBeanProxy$Visitor.visitAttribute(MXBeanProxy.java:63)\n",
      "\tat java.management/com.sun.jmx.mbeanserver.MBeanAnalyzer.visit(MBeanAnalyzer.java:68)\n",
      "\tat java.management/com.sun.jmx.mbeanserver.MXBeanProxy.<init>(MXBeanProxy.java:60)\n",
      "\tat java.management/javax.management.MBeanServerInvocationHandler.findMXBeanProxy(MBeanServerInvocationHandler.java:329)\n",
      "\tat java.management/javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:257)\n",
      "\tat com.sun.proxy.$Proxy10.getMemoryUsed(Unknown Source)\n",
      "\tat org.apache.spark.metrics.MBeanExecutorMetricType.getMetricValue(ExecutorMetricType.scala:67)\n",
      "\tat org.apache.spark.metrics.SingleValueExecutorMetricType.getMetricValues(ExecutorMetricType.scala:46)\n",
      "\tat org.apache.spark.metrics.SingleValueExecutorMetricType.getMetricValues$(ExecutorMetricType.scala:44)\n",
      "\tat org.apache.spark.metrics.MBeanExecutorMetricType.getMetricValues(ExecutorMetricType.scala:60)\n",
      "\tat org.apache.spark.executor.ExecutorMetrics$.$anonfun$getCurrentMetrics$1(ExecutorMetrics.scala:103)\n",
      "\tat org.apache.spark.executor.ExecutorMetrics$.$anonfun$getCurrentMetrics$1$adapted(ExecutorMetrics.scala:102)\n",
      "\tat org.apache.spark.executor.ExecutorMetrics$$$Lambda$3460/0x0000000801535840.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.executor.ExecutorMetrics$.getCurrentMetrics(ExecutorMetrics.scala:102)\n",
      "\tat org.apache.spark.SparkContext.reportHeartBeat(SparkContext.scala:2588)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$26(SparkContext.scala:574)\n",
      "\tat org.apache.spark.SparkContext$$Lambda$648/0x0000000800644840.apply$mcV$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "22/01/21 17:25:45 ERROR Utils: uncaught error in thread spark-listener-group-appStatus, stopping SparkContext\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:172)\n",
      "\tat java.base/java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:538)\n",
      "\tat java.base/java.lang.StringBuilder.append(StringBuilder.java:174)\n",
      "\tat scala.collection.mutable.StringBuilder.append(StringBuilder.scala:203)\n",
      "\tat scala.collection.TraversableOnce$appender$1.apply(TraversableOnce.scala:419)\n",
      "\tat scala.collection.TraversableOnce$appender$1.apply(TraversableOnce.scala:410)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.addString(TraversableOnce.scala:424)\n",
      "\tat scala.collection.TraversableOnce.addString$(TraversableOnce.scala:407)\n",
      "\tat scala.collection.AbstractIterator.addString(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.mkString(TraversableOnce.scala:377)\n",
      "\tat scala.collection.TraversableOnce.mkString$(TraversableOnce.scala:376)\n",
      "\tat scala.collection.AbstractIterator.mkString(Iterator.scala:1431)\n",
      "\tat scala.runtime.ScalaRunTime$._toString(ScalaRunTime.scala:165)\n",
      "\tat org.apache.spark.scheduler.AccumulableInfo.toString(AccumulableInfo.scala:41)\n",
      "\tat java.base/java.lang.String.valueOf(String.java:2951)\n",
      "\tat scala.collection.mutable.StringBuilder.append(StringBuilder.scala:203)\n",
      "\tat scala.collection.TraversableOnce$appender$1.apply(TraversableOnce.scala:419)\n",
      "\tat scala.collection.TraversableOnce$appender$1.apply(TraversableOnce.scala:410)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableOnce.addString(TraversableOnce.scala:424)\n",
      "\tat scala.collection.TraversableOnce.addString$(TraversableOnce.scala:407)\n",
      "\tat scala.collection.AbstractTraversable.addString(Traversable.scala:108)\n",
      "\tat scala.collection.TraversableOnce.mkString(TraversableOnce.scala:377)\n",
      "22/01/21 17:25:45 ERROR Utils: throw uncaught fatal error in thread spark-listener-group-appStatus\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:172)\n",
      "\tat java.base/java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:538)\n",
      "\tat java.base/java.lang.StringBuilder.append(StringBuilder.java:174)\n",
      "\tat scala.collection.mutable.StringBuilder.append(StringBuilder.scala:203)\n",
      "\tat scala.collection.TraversableOnce$appender$1.apply(TraversableOnce.scala:419)\n",
      "\tat scala.collection.TraversableOnce$appender$1.apply(TraversableOnce.scala:410)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.addString(TraversableOnce.scala:424)\n",
      "\tat scala.collection.TraversableOnce.addString$(TraversableOnce.scala:407)\n",
      "\tat scala.collection.AbstractIterator.addString(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.mkString(TraversableOnce.scala:377)\n",
      "\tat scala.collection.TraversableOnce.mkString$(TraversableOnce.scala:376)\n",
      "\tat scala.collection.AbstractIterator.mkString(Iterator.scala:1431)\n",
      "\tat scala.runtime.ScalaRunTime$._toString(ScalaRunTime.scala:165)\n",
      "\tat org.apache.spark.scheduler.AccumulableInfo.toString(AccumulableInfo.scala:41)\n",
      "\tat java.base/java.lang.String.valueOf(String.java:2951)\n",
      "\tat scala.collection.mutable.StringBuilder.append(StringBuilder.scala:203)\n",
      "\tat scala.collection.TraversableOnce$appender$1.apply(TraversableOnce.scala:419)\n",
      "\tat scala.collection.TraversableOnce$appender$1.apply(TraversableOnce.scala:410)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableOnce.addString(TraversableOnce.scala:424)\n",
      "\tat scala.collection.TraversableOnce.addString$(TraversableOnce.scala:407)\n",
      "\tat scala.collection.AbstractTraversable.addString(Traversable.scala:108)\n",
      "\tat scala.collection.TraversableOnce.mkString(TraversableOnce.scala:377)\n",
      "Exception in thread \"spark-listener-group-appStatus\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:172)\n",
      "\tat java.base/java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:538)\n",
      "\tat java.base/java.lang.StringBuilder.append(StringBuilder.java:174)\n",
      "\tat scala.collection.mutable.StringBuilder.append(StringBuilder.scala:203)\n",
      "\tat scala.collection.TraversableOnce$appender$1.apply(TraversableOnce.scala:419)\n",
      "\tat scala.collection.TraversableOnce$appender$1.apply(TraversableOnce.scala:410)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.addString(TraversableOnce.scala:424)\n",
      "\tat scala.collection.TraversableOnce.addString$(TraversableOnce.scala:407)\n",
      "\tat scala.collection.AbstractIterator.addString(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.mkString(TraversableOnce.scala:377)\n",
      "\tat scala.collection.TraversableOnce.mkString$(TraversableOnce.scala:376)\n",
      "\tat scala.collection.AbstractIterator.mkString(Iterator.scala:1431)\n",
      "\tat scala.runtime.ScalaRunTime$._toString(ScalaRunTime.scala:165)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tat org.apache.spark.scheduler.AccumulableInfo.toString(AccumulableInfo.scala:41)\n",
      "\tat java.base/java.lang.String.valueOf(String.java:2951)\n",
      "\tat scala.collection.mutable.StringBuilder.append(StringBuilder.scala:203)\n",
      "\tat scala.collection.TraversableOnce$appender$1.apply(TraversableOnce.scala:419)\n",
      "\tat scala.collection.TraversableOnce$appender$1.apply(TraversableOnce.scala:410)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableOnce.addString(TraversableOnce.scala:424)\n",
      "\tat scala.collection.TraversableOnce.addString$(TraversableOnce.scala:407)\n",
      "\tat scala.collection.AbstractTraversable.addString(Traversable.scala:108)\n",
      "\tat scala.collection.TraversableOnce.mkString(TraversableOnce.scala:377)\n",
      "22/01/21 17:25:46 ERROR Executor: Exception in task 5.0 in stage 25.0 (TID 80)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/01/21 17:25:46 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 5.0 in stage 25.0 (TID 80),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "22/01/21 17:25:46 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$4344/0x0000000801439040@7d6c7242 rejected from java.util.concurrent.ThreadPoolExecutor@26699a5e[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 75]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:137)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:817)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:791)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/01/21 17:25:47 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@5766031f rejected from java.util.concurrent.ThreadPoolExecutor@7176b370[Shutting down, pool size = 8, active threads = 8, queued tasks = 0, completed tasks = 75]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:270)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o109.fit.\n: org.apache.spark.SparkException: Job 10 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1115)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1113)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1113)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2615)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2515)\n\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2086)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1442)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2086)\n\tat org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2035)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/px/khc4q1n94cnblzp3k7s518xm0000gn/T/ipykernel_36253/1237242004.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit and transform the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpiped_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpapers_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_df_for_ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_df_for_ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o109.fit.\n: org.apache.spark.SparkException: Job 10 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1115)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1113)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1113)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2615)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2515)\n\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2086)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1442)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2086)\n\tat org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2035)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/21 17:25:52 ERROR Utils: Uncaught exception in thread Executor task launch worker for task 1.0 in stage 25.0 (TID 76)\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.scheduler.Task.$anonfun$run$2(Task.scala:152)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1442)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:150)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/01/21 17:25:52 ERROR Executor: Exception in task 1.0 in stage 25.0 (TID 76): Java heap space\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the data\n",
    "piped_data = papers_pipe.fit(final_df_for_ml).transform(final_df_for_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
