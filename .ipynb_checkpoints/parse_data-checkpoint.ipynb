{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e096de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pandas as pd;\n",
    "import csv;\n",
    "import numpy as np;\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "import re\n",
    "from pyspark.sql.types import StringType, ArrayType,StructType,StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600b2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('Read & parse text data file in pyspark')\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e0ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_KEYS_MAP = {\n",
    "    \"#index\": \"paper_id\", \"#*\": \"title\", \"#@\": \"authors\", \"#o\": \"affiliations\", \"#t\": \"year\", \"#c\": \"publication_venue\", \"#%\": \"ref_ids\", \"#!\": \"abstract\",\n",
    "}\n",
    "PAPER_DATASET_PATH = './assets/AMiner-Paper.txt'\n",
    "\n",
    "AUTHOR_KEYS_MAP = {\n",
    "    \"#index\": \"id\", \"#n\": \"name\", \"#a\": \"affiliations\", \"#pc\": \"paper_count\", \"#cn\": \"citation_count\", \"#t\": \"research_interests\", \"#hi\": \"h_index\", \"#upi\": \"u_p_index\", \"#pi\": \"p_index\"\n",
    "}\n",
    "AUTHOR_DATASET_PATH = './assets/AMiner-Author.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ea5547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[3] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "def zipDatasetWithIndex(datasetPath):\n",
    "    lines = sc.textFile(datasetPath).zipWithIndex()\n",
    "    return lines\n",
    "lines = zipDatasetWithIndex(PAPER_DATASET_PATH)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b305f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exchangeElementAtIndexToOne(index, zeroArray):\n",
    "    zeroArray[index] = 1;\n",
    "\n",
    "def doCumSumForIndexRows(lines):\n",
    "    # Find the starting row of each data entry\n",
    "    pos = lines.filter(lambda x: \"#index\" in x[0]).map(lambda x: x[1]).collect() \n",
    "    zeroArray = np.zeros(lines.count(), dtype=int)\n",
    "    for element in pos:\n",
    "        exchangeElementAtIndexToOne(element, zeroArray)\n",
    "    # Calculate cumulative sum of starting data entry indexes\n",
    "    summedArray = np.cumsum(zeroArray)\n",
    "    return summedArray;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5811092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summedArray = doCumSumForIndexRows(lines)\n",
    "# print(summedArray.collect())\n",
    "# Find the starting row of each data entry\n",
    "# pos = lines.filter(lambda x: \"#index\" in x[0]).map(lambda x: x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63589899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBroarcastedTuple(x, arrayOfStartRowIndexes):\n",
    "    return (arrayOfStartRowIndexes.value[x[1]], x[0])\n",
    "\n",
    "def convertDataIntoIndexedTuples(datasetLines, cumSummedIndexRowsArray):\n",
    "    broadcastedArray = sc.broadcast(cumSummedIndexRowsArray);\n",
    "    convertedData = datasetLines.map(lambda dataLine: createBroarcastedTuple(dataLine, broadcastedArray))\n",
    "    return convertedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aacf1ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_get = lambda l, x, d=None: d if not l[x] else l[x] # safe getter of list values\n",
    "\n",
    "def splitIntoKeyValue(stringToSplit, keyMap):\n",
    "    formattedValue = stringToSplit \n",
    "    if type(stringToSplit) is str:\n",
    "        if len(formattedValue) > 0:\n",
    "            formattedValue = formattedValue.split(\" \", 1)\n",
    "            key = list_get(formattedValue, 0)\n",
    "            mappedKey = keyMap.get(key)\n",
    "            if (mappedKey):\n",
    "                formattedValue = { mappedKey: list_get(formattedValue, 1, '') }\n",
    "            else:\n",
    "                formattedValue = {}\n",
    "        else:\n",
    "            formattedValue = {}\n",
    "    return formattedValue\n",
    "\n",
    "def appendStringOrListIntoList(lst, elToAppend):\n",
    "    if elToAppend is not None:\n",
    "        if type(elToAppend) == str:\n",
    "            lst.append(elToAppend)\n",
    "        else:\n",
    "            lst = lst + elToAppend\n",
    "    return lst\n",
    "\n",
    "# Paper related\n",
    "def convertFeatures(dct, affiliations_data={}, papers_data={}, paper_authors_data={}, publication_venues_data={}):\n",
    "    if dct.get(\"paper_id\"):\n",
    "        paper_id = dct.get(\"paper_id\")\n",
    "        affiliations_data[\"paper_id\"] = paper_id\n",
    "        papers_data[\"paper_id\"] = paper_id\n",
    "        paper_authors_data[\"paper_id\"] = paper_id\n",
    "        publication_venues_data[\"paper_id\"] = paper_id\n",
    "    elif dct.get(\"affiliations\"):\n",
    "        affiliations_data[\"affiliations\"] = dct.get(\"affiliations\")\n",
    "    elif dct.get(\"authors\"):\n",
    "        paper_authors_data[\"authors\"] = dct.get(\"authors\")\n",
    "    elif dct.get(\"title\"):\n",
    "        papers_data[\"title\"] = dct.get(\"title\")\n",
    "    elif dct.get(\"year\"):\n",
    "        papers_data[\"year\"] = dct.get(\"year\")\n",
    "    elif dct.get(\"publication_venue\"):\n",
    "        publication_venues_data[\"publication_venue\"] = dct.get(\"publication_venue\")\n",
    "        \n",
    "# Paper related\n",
    "def reduceFeaturesToDict(featureA='', featureB='', keyMap={}):\n",
    "    papers_data = {}\n",
    "    affiliations_data = {}\n",
    "    paper_authors_data = {}\n",
    "    publication_venues_data = {}\n",
    "    try:        \n",
    "        splittedA = splitIntoKeyValue(featureA, keyMap);\n",
    "        splittedB = splitIntoKeyValue(featureB, keyMap);\n",
    "#         print(\"===> \", splittedA, splittedB)\n",
    "        \n",
    "        if (splittedA.get('papers_data')):\n",
    "#             print(\"here\")\n",
    "            papers_data = splittedA.get('papers_data')\n",
    "            affiliations_data = splittedA.get('affiliations_data')\n",
    "        else:\n",
    "            convertFeatures(splittedA, affiliations_data, papers_data, paper_authors_data, publication_venues_data)\n",
    "        convertFeatures(splittedB, affiliations_data, papers_data, paper_authors_data, publication_venues_data)\n",
    "#         print(\"AFTER -> \", affiliations_data, papers_data)\n",
    "#         splittedARefIds = splittedA.get(\"ref_ids\");\n",
    "#         splittedBRefIds = splittedB.get(\"ref_ids\");\n",
    "#         newRefIds = [] \n",
    "#         appendStringOrListIntoList(newRefIds, splittedARefIds)\n",
    "#         appendStringOrListIntoList(newRefIds, splittedBRefIds)\n",
    "        # \"ref_ids\": ';'.join(newRefIds) \n",
    "#         print(\"Came to the end\")\n",
    "#         print(\"papers ---> \", papers_data, splittedA, splittedB)\n",
    "        return {\n",
    "            \"papers_data\": papers_data,\n",
    "            \"affiliations_data\": affiliations_data,\n",
    "            \"paper_authors_data\": paper_authors_data,\n",
    "            \"publication_venues_data\": publication_venues_data,\n",
    "        }\n",
    "    except Exception as error:\n",
    "        print(\"ERROR: \", featureA, featureB, error)\n",
    "        raise\n",
    "\n",
    "def convertDataIntoDicts(objects, dataKeyMap):\n",
    "    reducedObjects = objects.reduceByKey(lambda a, b: reduceFeaturesToDict(a, b, dataKeyMap))\n",
    "    print(reducedObjects.sortByKey(ascending=False).first())\n",
    "    mappedDicts = reducedObjects.map(lambda x: x[1]) # retrieve dicts from the tuples\n",
    "    return mappedDicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "615a9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDictsArrayIntoCSVFile(dictsArray, fileName):\n",
    "    df = sqlContext.createDataFrame(dictsArray)\n",
    "    # Save data to csv file\n",
    "    df.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "946bedef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "PythonRDD does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/px/khc4q1n94cnblzp3k7s518xm0000gn/T/ipykernel_66134/1420348471.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpapers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvertDataIntoIndexedTuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/px/khc4q1n94cnblzp3k7s518xm0000gn/T/ipykernel_66134/1191254387.py\u001b[0m in \u001b[0;36mconvertDataIntoIndexedTuples\u001b[0;34m(datasetLines, cumSummedIndexRowsArray)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvertDataIntoIndexedTuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetLines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcumSummedIndexRowsArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mbroadcastedArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcumSummedIndexRowsArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mconvertedData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasetLines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mdataLine\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcreateBroarcastedTuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataLine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcastedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconvertedData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36mbroadcast\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0mbe\u001b[0m \u001b[0msent\u001b[0m \u001b[0mto\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0monly\u001b[0m \u001b[0monce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \"\"\"\n\u001b[0;32m--> 990\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pickled_broadcast_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccum_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/broadcast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sc, value, pickle_registry, path, sock_file)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_broadcast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetupBroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encryption_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;31m# with encryption, we ask the jvm to do the encryption for us, we send it data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1708\u001b[0m             message = compute_exception_message(\n\u001b[1;32m   1709\u001b[0m                 \"{0} does not exist in the JVM\".format(name), error_message)\n\u001b[0;32m-> 1710\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: PythonRDD does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "papers = convertDataIntoIndexedTuples(lines, summedArray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfcac749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/px/khc4q1n94cnblzp3k7s518xm0000gn/T/ipykernel_66134/3455130164.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mauthorDictsArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvertDataIntoDicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpapers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPAPER_KEYS_MAP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/px/khc4q1n94cnblzp3k7s518xm0000gn/T/ipykernel_66134/961478021.py\u001b[0m in \u001b[0;36mconvertDataIntoDicts\u001b[0;34m(objects, dataKeyMap)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvertDataIntoDicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataKeyMap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mreducedObjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreduceFeaturesToDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataKeyMap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreducedObjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mmappedDicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreducedObjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# retrieve dicts from the tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduceByKey\u001b[0;34m(self, func, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   1891\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m         \"\"\"\n\u001b[0;32m-> 1893\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombineByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduceByKeyLocally\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcombineByKey\u001b[0;34m(self, createCombiner, mergeValue, mergeCombiners, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   2136\u001b[0m         \"\"\"\n\u001b[1;32m   2137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumPartitions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2138\u001b[0;31m             \u001b[0mnumPartitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_defaultReducePartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2140\u001b[0m         \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_defaultReducePartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2578\u001b[0m         \u001b[0mbe\u001b[0m \u001b[0minherent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2579\u001b[0m         \"\"\"\n\u001b[0;32m-> 2580\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.default.parallelism\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2581\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "authorDictsArray = convertDataIntoDicts(papers, PAPER_KEYS_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08815e8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve RDD 57\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n"
     ]
    }
   ],
   "source": [
    "papers_d = authorDictsArray.map(lambda x: x[\"papers_data\"])#.collect()\n",
    "affiliations_d = authorDictsArray.map(lambda x: x[\"affiliations_data\"])#.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d510255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(affiliations_d[1])\n",
    "print(papers_d[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "977a9b69",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/px/khc4q1n94cnblzp3k7s518xm0000gn/T/ipykernel_65557/2590400964.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print(authorDictsArray.toDF(['papers_data', 'affiliations_data'])[\"papers_data\"].show())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpapers_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maffiliations_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mPy4JJavaError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \"\"\"\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregisterDataFrameAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[1;32m    674\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can not infer schema from empty dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[0;31m# same type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m         \u001b[0mnfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()),\n\u001b[1;32m   1110\u001b[0m                                                   name=new_name(f.name)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print(authorDictsArray.toDF(['papers_data', 'affiliations_data'])[\"papers_data\"].show())\n",
    "# df1 = sqlContext.createDataFrame(papers_d)\n",
    "# df2 = sqlContext.createDataFrame(affiliations_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c70f9607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+-------+---+--------------------+-------+-----------+-------+--------------------+---------+\n",
      "|        affiliations|citation_count|h_index| id|                name|p_index|paper_count|ref_ids|  research_interests|u_p_index|\n",
      "+--------------------+--------------+-------+---+--------------------+-------+-----------+-------+--------------------+---------+\n",
      "|                    |             0|      0| 17|     J. Michael Howe| 0.0000|          1|       |HIV disease;Inter...|   0.0000|\n",
      "|University of Flo...|             0|      0| 34|        Haitham Gabr| 0.0000|          2|       |associate polynom...|   0.0000|\n",
      "|University of Bat...|             4|      1| 51|         Emma Tonkin| 3.0000|          8|       |metadata element;...|   3.5000|\n",
      "|School of Compute...|             1|      1| 68|        Woochul Shin| 0.5000|          4|       |Web Service;conte...|   0.7500|\n",
      "|                    |             0|      0| 85|           S Improta| 0.0000|          1|       |intermediate key;...|   0.0000|\n",
      "|                    |             8|      2|102|       Richard Ferri| 3.0000|          5|       |feedback loop;dif...|   3.7145|\n",
      "|                    |             0|      0|119|            Qing Liu| 0.0000|          1|       |Rough Set;nomal C...|   0.0000|\n",
      "|                    |             0|      0|136|      Artur Gramacki| 0.0000|          2|       |MATLAB toolbox;li...|   0.0000|\n",
      "|Department of Com...|             0|      0|153|Olumuyiwa Oluwasanmi| 0.0000|          2|       |Byzantine agreeme...|   0.0000|\n",
      "|                    |             0|      0|170|    Josef Willenborg| 0.0000|          1|       |Ein objektorienti...|   0.0000|\n",
      "|                    |             0|      0|187|            Qing Wei| 0.0000|          1|       |portable device;A...|   0.0000|\n",
      "|                    |             4|      1|204|Jurey Ivanovich Z...| 2.0000|          1|       |Integer-valued pr...|   1.0000|\n",
      "|Department of Com...|            13|      1|221|             Anny Ng| 6.5000|          1|       |stock price;stock...|   9.7500|\n",
      "|                    |             2|      1|238|    Nikos B. Pronios| 1.0000|          3|       |Hypermedia Synchr...|   1.0000|\n",
      "|                    |             0|      0|255| Lourdes Fraga Alman| 0.0000|          1|       |computer-mediated...|   0.0000|\n",
      "|Graduate School o...|             2|      1|272|       Junji Nishino| 0.3250|          7|       |Dijkstra method;o...|   0.4964|\n",
      "|LAII–ESIP (UPRES ...|             0|      0|289|       L. Dascalescu| 0.0000|          2|       |low-frequency act...|   0.0000|\n",
      "|                    |             0|      0|306|    Masakazu Nishino| 0.0000|          1|       |copyright process...|   0.0000|\n",
      "|Lockheed Martin C...|             4|      1|323|    Jean-Rémi Duquet| 1.3333|          2|       |uncertain informa...|   1.1111|\n",
      "|The Pennsylvania ...|             1|      1|340|     Brian A. Canada| 0.2000|          2|       |histology image;s...|   0.4567|\n",
      "+--------------------+--------------+-------+---+--------------------+-------+-----------+-------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df1.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347d9e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "convertDictsArrayIntoCSVFile(papers_d, \"./assets/papers_d2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b4f2e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "convertDictsArrayIntoCSVFile(affiliations_d, \"./assets/affiliations_d2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
