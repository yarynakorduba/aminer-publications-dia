{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e096de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pandas as pd;\n",
    "import csv;\n",
    "import numpy as np;\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "import re\n",
    "from pyspark.sql.types import StringType, ArrayType,StructType,StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600b2666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName('Read & parse text data file in pyspark')\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sqlContext = SQLContext.getOrCreate(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e0ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_KEYS_MAP = {\n",
    "    \"#index\": \"paper_id\",\n",
    "    \"#*\": \"title\",\n",
    "    \"#@\": \"authors\",\n",
    "    \"#o\": \"affiliations\",\n",
    "    \"#t\": \"year\",\n",
    "    \"#c\": \"publication_venue\",\n",
    "    \"#%\": \"ref_ids\",\n",
    "    \"#!\": \"abstract\",\n",
    "}\n",
    "PAPER_DATASET_PATH = './assets/AMiner-Paper.txt'\n",
    "\n",
    "AUTHOR_KEYS_MAP = {\n",
    "    \"#index\": \"author_id\",\n",
    "    \"#n\": \"name\",\n",
    "    \"#pc\": \"paper_count\",\n",
    "    \"#cn\": \"citation_count\",\n",
    "    \"#t\": \"research_interests\",\n",
    "    \"#hi\": \"h_index\", \n",
    "#     \"#upi\": \"u_p_index\", \n",
    "#     \"#pi\": \"p_index\"\n",
    "#     \"#a\": \"affiliations\",\n",
    "}\n",
    "AUTHOR_DATASET_PATH = './assets/AMiner-Author.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ea5547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipDatasetWithIndex(datasetPath):\n",
    "    lines = sc.textFile(datasetPath).zipWithIndex()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b305f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exchangeElementAtIndexToOne(index, zeroArray):\n",
    "    zeroArray[index] = 1;\n",
    "\n",
    "def doCumSumForIndexRows(lines):\n",
    "    # Find the starting row of each data entry\n",
    "    pos = lines.filter(lambda x: \"#index\" in x[0]).map(lambda x: x[1]).collect() \n",
    "    zeroArray = np.zeros(lines.count(), dtype=int)\n",
    "    for element in pos:\n",
    "        exchangeElementAtIndexToOne(element, zeroArray)\n",
    "    # Calculate cumulative sum of starting data entry indexes\n",
    "    summedArray = np.cumsum(zeroArray)\n",
    "    return summedArray;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63589899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBroarcastedTuple(x, arrayOfStartRowIndexes):\n",
    "    return (arrayOfStartRowIndexes.value[x[1]], x[0])\n",
    "\n",
    "def convertDataIntoIndexedTuples(datasetLines, cumSummedIndexRowsArray):\n",
    "    broadcastedArray = sc.broadcast(cumSummedIndexRowsArray);\n",
    "    convertedData = datasetLines.map(lambda dataLine: createBroarcastedTuple(dataLine, broadcastedArray))\n",
    "    return convertedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33beefa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_get = lambda l, x, d=None: d if not l[x] else l[x] # safe getter of list values\n",
    "\n",
    "def splitIntoKeyValue(stringToSplit, keyMap):\n",
    "    formattedValue = stringToSplit \n",
    "    if type(stringToSplit) is str:\n",
    "        if len(formattedValue) > 0:\n",
    "            formattedValue = formattedValue.split(\" \", 1)\n",
    "            key = list_get(formattedValue, 0)\n",
    "            mappedKey = keyMap.get(key)\n",
    "            if (mappedKey):\n",
    "                formattedValue = { mappedKey: list_get(formattedValue, 1, '') }\n",
    "            else:\n",
    "                formattedValue = {}\n",
    "        else:\n",
    "            formattedValue = {}\n",
    "    return formattedValue\n",
    "\n",
    "def appendStringOrListIntoList(lst, elToAppend):\n",
    "    if elToAppend is not None:\n",
    "        if type(elToAppend) == str:\n",
    "            lst.append(elToAppend)\n",
    "        else:\n",
    "            lst = lst + elToAppend\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7623d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper related\n",
    "def convertPaperFeatures(dct, affiliations_data={}, papers_data={}, paper_authors_data={}, publication_venues_data={}, refs_data={}):\n",
    "    if dct.get(\"paper_id\"):\n",
    "        paper_id = dct.get(\"paper_id\")   \n",
    "        affiliations_data[\"paper_id\"] = paper_id\n",
    "        papers_data[\"paper_id\"] = paper_id\n",
    "        paper_authors_data[\"paper_id\"] = paper_id\n",
    "        publication_venues_data[\"paper_id\"] = paper_id\n",
    "        refs_data[\"paper_id\"] = paper_id\n",
    "    elif dct.get(\"affiliations\"):\n",
    "        affiliations_data[\"affiliations\"] = dct.get(\"affiliations\")\n",
    "    elif dct.get(\"ref_ids\"):\n",
    "        appendStringOrListIntoList(refs_data[\"ref_ids\"], dct.get(\"ref_ids\"))\n",
    "    elif dct.get(\"authors\"):\n",
    "        paper_authors_data[\"authors\"] = dct.get(\"authors\")\n",
    "    elif dct.get(\"title\"):\n",
    "        papers_data[\"title\"] = dct.get(\"title\")\n",
    "    elif dct.get(\"year\"):\n",
    "        papers_data[\"year\"] = dct.get(\"year\")\n",
    "    elif dct.get(\"publication_venue\"):\n",
    "        publication_venues_data[\"publication_venue\"] = dct.get(\"publication_venue\")\n",
    "        \n",
    "# Paper related\n",
    "def reducePaperFeaturesToDict(featureA='', featureB='', keyMap={}):\n",
    "    papers_data = {}\n",
    "    affiliations_data = {}\n",
    "    paper_authors_data = {}\n",
    "    publication_venues_data = {}\n",
    "    refs_data = { \"ref_ids\": [] }\n",
    "    try:        \n",
    "        splittedA = splitIntoKeyValue(featureA, keyMap);\n",
    "        splittedB = splitIntoKeyValue(featureB, keyMap);\n",
    "        \n",
    "        if (splittedA.get('papers_data')):\n",
    "            papers_data = splittedA.get('papers_data')\n",
    "            affiliations_data = splittedA.get('affiliations_data')\n",
    "            paper_authors_data = splittedA.get('paper_authors_data')\n",
    "            publication_venues_data = splittedA.get('publication_venues_data')\n",
    "            refs_data = splittedA.get('refs_data')\n",
    "        else:\n",
    "            convertPaperFeatures(splittedA, affiliations_data, papers_data, paper_authors_data, publication_venues_data, refs_data)\n",
    "        convertPaperFeatures(splittedB, affiliations_data, papers_data, paper_authors_data, publication_venues_data, refs_data)\n",
    "        return {\n",
    "            \"papers_data\": papers_data,\n",
    "            \"affiliations_data\": affiliations_data,\n",
    "            \"paper_authors_data\": paper_authors_data,\n",
    "            \"publication_venues_data\": publication_venues_data,\n",
    "            \"refs_data\": refs_data\n",
    "        }\n",
    "    except Exception as error:\n",
    "        print(\"ERROR: \", featureA, featureB, error)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aacf1ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author related\n",
    "def convertAuthorFeatures(dct, authors_data={}, research_interests_data={}):\n",
    "    if dct.get(\"author_id\"):\n",
    "        author_id = dct.get(\"author_id\")   \n",
    "        authors_data[\"author_id\"] = author_id\n",
    "        research_interests_data[\"author_id\"] = author_id\n",
    "    elif dct.get(\"name\"):\n",
    "        authors_data[\"name\"] = dct.get(\"name\")\n",
    "    elif dct.get(\"paper_count\"):\n",
    "        authors_data[\"paper_count\"] = dct.get(\"paper_count\")\n",
    "    elif dct.get(\"сitation_count\"):\n",
    "        authors_data[\"сitation_count\"] = dct.get(\"сitation_count\")\n",
    "    elif dct.get(\"research_interests\"):\n",
    "        research_interests_data[\"research_interests\"] = dct.get(\"research_interests\")\n",
    "    elif dct.get(\"h_index\"):\n",
    "        authors_data[\"h_index\"] = dct.get(\"h_index\")\n",
    "    elif dct.get(\"publication_venue\"):\n",
    "        authors_data[\"publication_venue\"] = dct.get(\"publication_venue\")\n",
    "        \n",
    "# Author related\n",
    "def reduceAuthorFeaturesToDict(featureA='', featureB='', keyMap={}):\n",
    "    authors_data = {}\n",
    "    research_interests_data = {}\n",
    "    try:        \n",
    "        splittedA = splitIntoKeyValue(featureA, keyMap);\n",
    "        splittedB = splitIntoKeyValue(featureB, keyMap);\n",
    "        \n",
    "        if (splittedA.get('authors_data')):\n",
    "            authors_data = splittedA.get('authors_data')\n",
    "            research_interests_data = splittedA.get('research_interests_data')\n",
    "        else: # If it's first reduce run, accumulator === the first item of the list. So the item should be converted\n",
    "            convertAuthorFeatures(splittedA, authors_data, research_interests_data)\n",
    "        convertAuthorFeatures(splittedB, authors_data, research_interests_data)\n",
    "        return {\n",
    "            \"authors_data\": authors_data,\n",
    "            \"research_interests_data\": research_interests_data,\n",
    "        }\n",
    "    except Exception as error:\n",
    "        print(\"ERROR: \", featureA, featureB, error)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "615a9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDataIntoDicts(objects, dataKeyMap, reducer):\n",
    "    reducedObjects = objects.reduceByKey(lambda a, b: reducer(a, b, dataKeyMap))\n",
    "    print(reducedObjects.sortByKey(ascending=False).first())\n",
    "    mappedDicts = reducedObjects.map(lambda x: x[1]) # retrieve dicts from the tuples\n",
    "    return mappedDicts\n",
    "\n",
    "def convertDictsArrayIntoCSVFile(dictsArray, fileName):\n",
    "    df = sqlContext.createDataFrame(dictsArray)\n",
    "    print(df.show())\n",
    "    # Save data to csv file\n",
    "    df.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "946bedef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'c'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/px/khc4q1n94cnblzp3k7s518xm0000gn/T/ipykernel_81339/3910472830.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpaperTextLines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipDatasetWithIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPAPER_DATASET_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/px/khc4q1n94cnblzp3k7s518xm0000gn/T/ipykernel_81339/1678425069.py\u001b[0m in \u001b[0;36mzipDatasetWithIndex\u001b[0;34m(datasetPath)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mzipDatasetWithIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzipWithIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36mtextFile\u001b[0;34m(self, name, minPartitions, use_unicode)\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m'Hello world!'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \"\"\"\n\u001b[0;32m--> 654\u001b[0;31m         \u001b[0mminPartitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminPartitions\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m         return RDD(self._jsc.textFile(name, minPartitions), self,\n\u001b[1;32m    656\u001b[0m                    UTF8Deserializer(use_unicode))\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36mdefaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    448\u001b[0m         reduce tasks)\n\u001b[1;32m    449\u001b[0m         \"\"\"\n\u001b[0;32m--> 450\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'c'"
     ]
    }
   ],
   "source": [
    "paperTextLines = zipDatasetWithIndex(PAPER_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5811092f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "PythonRDD does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/px/khc4q1n94cnblzp3k7s518xm0000gn/T/ipykernel_81339/362420424.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummedArrayOfPaperIndexRows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoCumSumForIndexRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaperTextLines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/px/khc4q1n94cnblzp3k7s518xm0000gn/T/ipykernel_81339/2038728549.py\u001b[0m in \u001b[0;36mdoCumSumForIndexRows\u001b[0;34m(lines)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdoCumSumForIndexRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Find the starting row of each data entry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"#index\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mzeroArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \"\"\"\n\u001b[1;32m    949\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1708\u001b[0m             message = compute_exception_message(\n\u001b[1;32m   1709\u001b[0m                 \"{0} does not exist in the JVM\".format(name), error_message)\n\u001b[0;32m-> 1710\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: PythonRDD does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "summedArrayOfPaperIndexRows = doCumSumForIndexRows(paperTextLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfa835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperItemTuples = convertDataIntoIndexedTuples(paperTextLines, summedArrayOfPaperIndexRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcac749",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperDictsArray = convertDataIntoDicts(paperItemTuples, PAPER_KEYS_MAP, reducePaperFeaturesToDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08815e8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mapPapersData(data):\n",
    "    refs_data = data[\"refs_data\"]\n",
    "    refs_data['ref_ids'] = ';'.join(refs_data['ref_ids'])\n",
    "    return refs_data\n",
    "\n",
    "papers_d = paperDictsArray.map(lambda x: x[\"papers_data\"])\n",
    "affiliations_d = paperDictsArray.map(lambda x: x[\"affiliations_data\"])\n",
    "paper_authors_d = paperDictsArray.map(lambda x: x[\"paper_authors_data\"])\n",
    "publication_venues_d = paperDictsArray.map(lambda x: x[\"publication_venues_data\"])\n",
    "paper_refs_d = paperDictsArray.map(lambda x:mapPapersData(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cf3c692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:=======================================================>(64 + 1) / 65]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2092356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(papers_d.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347d9e57",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convertDictsArrayIntoCSVFile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/px/khc4q1n94cnblzp3k7s518xm0000gn/T/ipykernel_81304/221289151.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# convertDictsArrayIntoCSVFile(paper_authors_d, \"./assets/paper_authors_d9.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# convertDictsArrayIntoCSVFile(publication_venues_d, \"./assets/publication_venues_d9.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mconvertDictsArrayIntoCSVFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper_refs_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./assets/paper_refs_d9.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'convertDictsArrayIntoCSVFile' is not defined"
     ]
    }
   ],
   "source": [
    "# convertDictsArrayIntoCSVFile(papers_d, \"./assets/papers_d9.csv\")\n",
    "# convertDictsArrayIntoCSVFile(affiliations_d, \"./assets/affiliations_d9.csv\")\n",
    "# convertDictsArrayIntoCSVFile(paper_authors_d, \"./assets/paper_authors_d9.csv\")\n",
    "# convertDictsArrayIntoCSVFile(publication_venues_d, \"./assets/publication_venues_d9.csv\")\n",
    "convertDictsArrayIntoCSVFile(paper_refs_d, \"./assets/paper_refs_d9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b4f2e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Authors\n",
    "authorTextLines = zipDatasetWithIndex(AUTHOR_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfd6a922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summedArrayOfAuthorIndexRows = doCumSumForIndexRows(authorTextLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a05b5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorItemTuples = convertDataIntoIndexedTuples(authorTextLines, summedArrayOfAuthorIndexRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffb1f0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, '#index 1'), (1, '#n O. Willum'), (1, '#a Res. Center for Microperipherik, Technische Univ. Berlin, Germany'), (1, '#pc 1'), (1, '#cn 0'), (1, '#hi 0'), (1, '#pi 0.0000'), (1, '#upi 0.0000'), (1, '#t new product;product group;active product;long product lifetime;old product;product generation;new technology;environmental benefit;environmental choice;environmental consequence'), (1, '')]\n"
     ]
    }
   ],
   "source": [
    "print(authorItemTuples.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e0e5de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "[Stage 19:====================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1712433, {'authors_data': {'author_id': '1712433', 'name': 'Andrea Gantchev', 'paper_count': '2', 'h_index': '1'}, 'research_interests_data': {'author_id': '1712433', 'research_interests': 'subsumption architecture;Subsumption ArchitectureThe subsumption architecture;software architecture;subsumption architectureReusable Strategies;Object-oriented design;object-oriented software design;Rodney Brooks;Software Agents;behaviour-based control;different micro-strategies'}})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "authorDictsArray = convertDataIntoDicts(authorItemTuples, AUTHOR_KEYS_MAP, reduceAuthorFeaturesToDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bed74803",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_d = authorDictsArray.map(lambda x: x[\"authors_data\"])\n",
    "research_interests_d = authorDictsArray.map(lambda x: x[\"research_interests_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc3f39af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------------------+-----------+\n",
      "|author_id|h_index|                name|paper_count|\n",
      "+---------+-------+--------------------+-----------+\n",
      "|       17|      0|     J. Michael Howe|          1|\n",
      "|       34|      0|        Haitham Gabr|          2|\n",
      "|       51|      1|         Emma Tonkin|          8|\n",
      "|       68|      1|        Woochul Shin|          4|\n",
      "|       85|      0|           S Improta|          1|\n",
      "|      102|      2|       Richard Ferri|          5|\n",
      "|      119|      0|            Qing Liu|          1|\n",
      "|      136|      0|      Artur Gramacki|          2|\n",
      "|      153|      0|Olumuyiwa Oluwasanmi|          2|\n",
      "|      170|      0|    Josef Willenborg|          1|\n",
      "|      187|      0|            Qing Wei|          1|\n",
      "|      204|      1|Jurey Ivanovich Z...|          1|\n",
      "|      221|      1|             Anny Ng|          1|\n",
      "|      238|      1|    Nikos B. Pronios|          3|\n",
      "|      255|      0| Lourdes Fraga Alman|          1|\n",
      "|      272|      1|       Junji Nishino|          7|\n",
      "|      289|      0|       L. Dascalescu|          2|\n",
      "|      306|      0|    Masakazu Nishino|          1|\n",
      "|      323|      1|    Jean-Rémi Duquet|          2|\n",
      "|      340|      1|     Brian A. Canada|          2|\n",
      "+---------+-------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|author_id|  research_interests|\n",
      "+---------+--------------------+\n",
      "|       17|HIV disease;Inter...|\n",
      "|       34|associate polynom...|\n",
      "|       51|metadata element;...|\n",
      "|       68|Web Service;conte...|\n",
      "|       85|intermediate key;...|\n",
      "|      102|feedback loop;dif...|\n",
      "|      119|Rough Set;nomal C...|\n",
      "|      136|MATLAB toolbox;li...|\n",
      "|      153|Byzantine agreeme...|\n",
      "|      170|Ein objektorienti...|\n",
      "|      187|portable device;A...|\n",
      "|      204|Integer-valued pr...|\n",
      "|      221|stock price;stock...|\n",
      "|      238|Hypermedia Synchr...|\n",
      "|      255|computer-mediated...|\n",
      "|      272|Dijkstra method;o...|\n",
      "|      289|low-frequency act...|\n",
      "|      306|copyright process...|\n",
      "|      323|uncertain informa...|\n",
      "|      340|histology image;s...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "convertDictsArrayIntoCSVFile(authors_d, \"./assets/authors_d8.csv\")\n",
    "convertDictsArrayIntoCSVFile(research_interests_d, \"./assets/research_interests_d8.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
