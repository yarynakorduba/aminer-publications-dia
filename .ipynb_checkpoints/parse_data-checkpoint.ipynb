{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf7a7327",
   "metadata": {},
   "source": [
    "# Data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf98b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /**\n",
    "# *                   _oo0oo_\n",
    "# *                  o8888888o\n",
    "# *                  88\" . \"88\n",
    "# *                  (| -_- |)\n",
    "# *                  0\\  =  /0\n",
    "# *                ___/`---'\\___\n",
    "# *              .' \\\\|     |// '.\n",
    "# *             / \\\\|||  :  |||// \\\n",
    "# *            / _||||| -:- |||||- \\\n",
    "# *           |   | \\\\\\  -  /// |   |\n",
    "# *           | \\_|  ''\\---/''  |_/ |\n",
    "# *           \\  .-\\__  '-'  ___/-. /\n",
    "# *         ___'. .'  /--.--\\  `. .'___\n",
    "# *      .\"\" '<  `.___\\_<|>_/___.' >' \"\".\n",
    "# *     | | :  `- \\`.;`\\ _ /`;.`/ - ` : | |\n",
    "# *     \\  \\ `_.   \\_ __\\ /__ _/   .-` /  /\n",
    "# * =====`-.____`.___ \\_____/___.-`___.-'=====\n",
    "# *                   `=---='\n",
    "# *\n",
    "# *\n",
    "# * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# *\n",
    "# *   Buddha blesses your code to be bug free\n",
    "# */"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaf3384",
   "metadata": {},
   "source": [
    "This file is used to parse the initial data from [AMiner dataset](https://www.aminer.org/aminernetwork). With this implementation we parse the text files (using cumulative sum applied to file lines) and split them into a couple of csvs which will later be used to create the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e096de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pandas as pd;\n",
    "import csv;\n",
    "import numpy as np;\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "import re\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from helpers import createDFFromFileAndSchema, saveDFIntoCSVFolder, moveFileToCorrectFolder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "600b2666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName('Read& parse text data file in pyspark')\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sqlContext = SQLContext.getOrCreate(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18e0ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_KEYS_MAP = {\n",
    "    \"#index\": \"paper_id\",\n",
    "    \"#*\": \"title\",\n",
    "    \"#@\": \"authors\",\n",
    "    \"#o\": \"affiliations\",\n",
    "    \"#t\": \"year\",\n",
    "    \"#c\": \"publication_venue\",\n",
    "    \"#%\": \"ref_ids\",\n",
    "    \"#!\": \"abstract\",\n",
    "}\n",
    "PAPER_DATASET_PATH = './assets/AMiner-Paper.txt'\n",
    "\n",
    "AUTHOR_KEYS_MAP = {\n",
    "    \"#index\": \"author_id\",\n",
    "    \"#n\": \"name\",\n",
    "    \"#pc\": \"paper_count\",\n",
    "    \"#cn\": \"citation_count\",\n",
    "    \"#t\": \"research_interests\",\n",
    "    \"#hi\": \"h_index\", \n",
    "}\n",
    "AUTHOR_DATASET_PATH = './assets/AMiner-Author.txt'\n",
    "\n",
    "AUTHOR_2_PAPER_DATASET_PATH = './assets/AMiner-Author2Paper.txt'\n",
    "AUTHOR_2_PAPER_SCHEMA_PATH = './schemas/paper_author_id.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2ea5547",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the file and zip each line into a tuple together with its index\n",
    "def zipDatasetWithIndex(datasetPath):\n",
    "    lines = sc.textFile(datasetPath).zipWithIndex()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c96a362",
   "metadata": {},
   "source": [
    "### Here we apply cumsum to find the row indexes with the data of each data entry\n",
    "1. Zero array: Create an array of 0s with the length equal to the number of lines in the file\n",
    "2. Find the indexes of the lines starting with `#index`\n",
    "3. Exchange those indexes inside Zero array into 1s.\n",
    "4. Apply cumulative sum to this array to find out entry row indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b305f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exchangeElementAtIndexToOne(index, zeroArray):\n",
    "    zeroArray[index] = 1;\n",
    "\n",
    "def doCumSumForIndexRows(lines):\n",
    "    # Find the starting row of each data entry\n",
    "    pos = lines.filter(lambda x: \"#index\" in x[0]).map(lambda x: x[1]).collect() \n",
    "    zeroArray = np.zeros(lines.count(), dtype=int)\n",
    "    for element in pos:\n",
    "        exchangeElementAtIndexToOne(element, zeroArray)\n",
    "    # Calculate cumulative sum of starting data entry indexes\n",
    "    summedArray = np.cumsum(zeroArray)\n",
    "    return summedArray;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63589899",
   "metadata": {},
   "outputs": [],
   "source": [
    "### These functions are used to gather the text lines for each data entry\n",
    "def createBroarcastedTuple(x, arrayOfStartRowIndexes):\n",
    "    return (arrayOfStartRowIndexes.value[x[1]], x[0])\n",
    "\n",
    "def convertDataIntoIndexedTuples(datasetLines, cumSummedIndexRowsArray):\n",
    "    broadcastedArray = sc.broadcast(cumSummedIndexRowsArray);\n",
    "    convertedData = datasetLines.map(lambda dataLine: createBroarcastedTuple(dataLine, broadcastedArray))\n",
    "    return convertedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33beefa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function is used to split string into key and value\n",
    "list_get = lambda l, x, d=None: d if not l[x] else l[x] # safe getter of list values\n",
    "\n",
    "def splitIntoKeyValue(stringToSplit, keyMap):\n",
    "    formattedValue = stringToSplit \n",
    "    if type(stringToSplit) is str:\n",
    "        if len(formattedValue) > 0:\n",
    "            formattedValue = formattedValue.split(\" \", 1)\n",
    "            key = list_get(formattedValue, 0)\n",
    "            mappedKey = keyMap.get(key)\n",
    "            if (mappedKey):\n",
    "                formattedValue = { mappedKey: list_get(formattedValue, 1, '') }\n",
    "            else:\n",
    "                formattedValue = {}\n",
    "        else:\n",
    "            formattedValue = {}\n",
    "    return formattedValue\n",
    "\n",
    "def appendStringOrListIntoList(lst, elToAppend):\n",
    "    if elToAppend is not None:\n",
    "        if type(elToAppend) == str:\n",
    "            lst.append(elToAppend)\n",
    "        else:\n",
    "            lst = lst + elToAppend\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7623d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "### These functios are used to convert the strings with paper features for a data entry\n",
    "### into a dictionnary with mapped key and value\n",
    "\n",
    "### Paper related\n",
    "def convertPaperFeatures(dct, affiliations_data={}, papers_data={}, paper_authors_data={}, publication_venues_data={}, refs_data={}):\n",
    "    if dct.get(\"paper_id\"):\n",
    "        paper_id = dct.get(\"paper_id\")   \n",
    "        affiliations_data[\"paper_id\"] = paper_id\n",
    "        papers_data[\"paper_id\"] = paper_id\n",
    "        paper_authors_data[\"paper_id\"] = paper_id\n",
    "        publication_venues_data[\"paper_id\"] = paper_id\n",
    "        refs_data[\"paper_id\"] = paper_id\n",
    "    elif dct.get(\"affiliations\"):\n",
    "        affiliations_data[\"affiliations\"] = dct.get(\"affiliations\")\n",
    "    elif dct.get(\"ref_ids\"):\n",
    "        appendStringOrListIntoList(refs_data[\"ref_ids\"], dct.get(\"ref_ids\"))\n",
    "    elif dct.get(\"authors\"):\n",
    "        paper_authors_data[\"authors\"] = dct.get(\"authors\")\n",
    "    elif dct.get(\"title\"):\n",
    "        papers_data[\"title\"] = dct.get(\"title\")\n",
    "    elif dct.get(\"year\"):\n",
    "        papers_data[\"year\"] = dct.get(\"year\")\n",
    "    elif dct.get(\"publication_venue\"):\n",
    "        publication_venues_data[\"publication_venue\"] = dct.get(\"publication_venue\")\n",
    "        \n",
    "# Paper related\n",
    "def reducePaperFeaturesToDict(featureA='', featureB='', keyMap={}):\n",
    "    papers_data = {}\n",
    "    affiliations_data = {}\n",
    "    paper_authors_data = {}\n",
    "    publication_venues_data = {}\n",
    "    refs_data = { \"ref_ids\": [] }\n",
    "    try:        \n",
    "        splittedA = splitIntoKeyValue(featureA, keyMap);\n",
    "        splittedB = splitIntoKeyValue(featureB, keyMap);\n",
    "        \n",
    "        if (splittedA.get('papers_data')):\n",
    "            papers_data = splittedA.get('papers_data')\n",
    "            affiliations_data = splittedA.get('affiliations_data')\n",
    "            paper_authors_data = splittedA.get('paper_authors_data')\n",
    "            publication_venues_data = splittedA.get('publication_venues_data')\n",
    "            refs_data = splittedA.get('refs_data')\n",
    "        else:\n",
    "            convertPaperFeatures(splittedA, affiliations_data, papers_data, paper_authors_data, publication_venues_data, refs_data)\n",
    "        convertPaperFeatures(splittedB, affiliations_data, papers_data, paper_authors_data, publication_venues_data, refs_data)\n",
    "        return {\n",
    "            \"papers_data\": papers_data,\n",
    "            \"affiliations_data\": affiliations_data,\n",
    "            \"paper_authors_data\": paper_authors_data,\n",
    "            \"publication_venues_data\": publication_venues_data,\n",
    "            \"refs_data\": refs_data\n",
    "        }\n",
    "    except Exception as error:\n",
    "        print(\"ERROR: \", featureA, featureB, error)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aacf1ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### These functios are used to convert the strings with author features for a data entry\n",
    "### into a dictionnary with mapped key and value\n",
    "\n",
    "# Author related\n",
    "def convertAuthorFeatures(dct, authors_data={}, research_interests_data={}):\n",
    "    if dct.get(\"author_id\"):\n",
    "        author_id = dct.get(\"author_id\")   \n",
    "        authors_data[\"author_id\"] = author_id\n",
    "        research_interests_data[\"author_id\"] = author_id\n",
    "    elif dct.get(\"name\"):\n",
    "        authors_data[\"name\"] = dct.get(\"name\")\n",
    "    elif dct.get(\"paper_count\"):\n",
    "        authors_data[\"paper_count\"] = dct.get(\"paper_count\")\n",
    "    elif dct.get(\"citation_count\"):\n",
    "        authors_data[\"citation_count\"] = dct.get(\"citation_count\")\n",
    "    elif dct.get(\"research_interests\"):\n",
    "        research_interests_data[\"research_interests\"] = dct.get(\"research_interests\")\n",
    "    elif dct.get(\"h_index\"):\n",
    "        authors_data[\"h_index\"] = dct.get(\"h_index\")\n",
    "    elif dct.get(\"publication_venue\"):\n",
    "        authors_data[\"publication_venue\"] = dct.get(\"publication_venue\")\n",
    "        \n",
    "# Author related\n",
    "def reduceAuthorFeaturesToDict(featureA='', featureB='', keyMap={}):\n",
    "    authors_data = {}\n",
    "    research_interests_data = {}\n",
    "    try:        \n",
    "        splittedA = splitIntoKeyValue(featureA, keyMap);\n",
    "        splittedB = splitIntoKeyValue(featureB, keyMap);\n",
    "        \n",
    "        if (splittedA.get('authors_data')):\n",
    "            authors_data = splittedA.get('authors_data')\n",
    "            research_interests_data = splittedA.get('research_interests_data')\n",
    "        else: # If it's first reduce run, accumulator === the first item of the list. So the item should be converted\n",
    "            convertAuthorFeatures(splittedA, authors_data, research_interests_data)\n",
    "        convertAuthorFeatures(splittedB, authors_data, research_interests_data)\n",
    "        return {\n",
    "            \"authors_data\": authors_data,\n",
    "            \"research_interests_data\": research_interests_data,\n",
    "        }\n",
    "    except Exception as error:\n",
    "        print(\"ERROR: \", featureA, featureB, error)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "615a9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDataIntoDicts(objects, dataKeyMap, reducer):\n",
    "    reducedObjects = objects.reduceByKey(lambda a, b: reducer(a, b, dataKeyMap))\n",
    "    print(reducedObjects.sortByKey(ascending=False).first())\n",
    "    mappedDicts = reducedObjects.map(lambda x: x[1]) # retrieve dicts from the tuples\n",
    "    return mappedDicts\n",
    "\n",
    "def convertDictsArrayIntoCSVFile(dictsArray, folderName, pathToFolder):\n",
    "    df = sqlContext.createDataFrame(dictsArray)\n",
    "    print(df.show())\n",
    "    # Save data to csv file\n",
    "    df.coalesce(1).write \\\n",
    "        .format(\"com.databricks.spark.csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .save(f'{pathToFolder}{folderName}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e10124",
   "metadata": {},
   "source": [
    "## Convert the papers dataset using above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "946bedef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "paperTextLines = zipDatasetWithIndex(PAPER_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5811092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summedArrayOfPaperIndexRows = doCumSumForIndexRows(paperTextLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bfa835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperItemTuples = convertDataIntoIndexedTuples(paperTextLines, summedArrayOfPaperIndexRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfcac749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2092356, {'papers_data': {'paper_id': '2092356', 'title': 'Reliability prediction through system modeling', 'year': '2013'}, 'affiliations_data': {'paper_id': '2092356', 'affiliations': 'Dept of Computer Engg IIT(BHU) Varanasi, India;Reactor Safety Division Bhabha Atomic Research Centre Dept of Atomic Energy, Govt of India;Dept of Computer Engg IIT(BHU) Varanasi, India'}, 'paper_authors_data': {'paper_id': '2092356', 'authors': 'Lalit Kumar Singh;Gopika Vinod;A. K. Tripathi'}, 'publication_venues_data': {'paper_id': '2092356', 'publication_venue': 'ACM SIGSOFT Software Engineering Notes'}, 'refs_data': {'ref_ids': ['215579', '333683', '511383', '594375', '641666', '763878', '966860', '1056157'], 'paper_id': '2092356'}})\n"
     ]
    }
   ],
   "source": [
    "paperDictsArray = convertDataIntoDicts(paperItemTuples, PAPER_KEYS_MAP, reducePaperFeaturesToDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08815e8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mapPapersData(data):\n",
    "    refs_data = data[\"refs_data\"]\n",
    "    refs_data['ref_ids'] = ';'.join(refs_data['ref_ids'])\n",
    "    return refs_data\n",
    "\n",
    "papers_d = paperDictsArray.map(lambda x: x[\"papers_data\"])\n",
    "affiliations_d = paperDictsArray.map(lambda x: x[\"affiliations_data\"])\n",
    "paper_authors_d = paperDictsArray.map(lambda x: x[\"paper_authors_data\"])\n",
    "publication_venues_d = paperDictsArray.map(lambda x: x[\"publication_venues_data\"])\n",
    "paper_refs_d = paperDictsArray.map(lambda x:mapPapersData(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8cf3c692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:================================================>       (56 + 8) / 65]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2092356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 63:=======================================================>(64 + 1) / 65]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(papers_d.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347d9e57",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PATH_TO_ASSETS = './assets/'\n",
    "convertDictsArrayIntoCSVFile(papers_d, \"papers\", PATH_TO_ASSETS)\n",
    "convertDictsArrayIntoCSVFile(affiliations_d, \"affiliations\", PATH_TO_ASSETS)\n",
    "convertDictsArrayIntoCSVFile(paper_authors_d, \"paper_authors\", PATH_TO_ASSETS)\n",
    "convertDictsArrayIntoCSVFile(publication_venues_d, \"publication_venues\", PATH_TO_ASSETS)\n",
    "convertDictsArrayIntoCSVFile(paper_refs_d, \"paper_refs\", PATH_TO_ASSETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef5c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieve the files into a correct folder\n",
    "moveFileToCorrectFolder('papers', PATH_TO_ASSETS)\n",
    "moveFileToCorrectFolder('affiliations', PATH_TO_ASSETS)\n",
    "moveFileToCorrectFolder('paper_authors', PATH_TO_ASSETS)\n",
    "moveFileToCorrectFolder('publication_venues', PATH_TO_ASSETS)\n",
    "moveFileToCorrectFolder('paper_refs', PATH_TO_ASSETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d998e",
   "metadata": {},
   "source": [
    "## Convert the authors dataset using above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0b4f2e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Authors\n",
    "authorTextLines = zipDatasetWithIndex(AUTHOR_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bfd6a922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summedArrayOfAuthorIndexRows = doCumSumForIndexRows(authorTextLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a05b5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorItemTuples = convertDataIntoIndexedTuples(authorTextLines, summedArrayOfAuthorIndexRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ffb1f0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, '#index 1'), (1, '#n O. Willum'), (1, '#a Res. Center for Microperipherik, Technische Univ. Berlin, Germany'), (1, '#pc 1'), (1, '#cn 0'), (1, '#hi 0'), (1, '#pi 0.0000'), (1, '#upi 0.0000'), (1, '#t new product;product group;active product;long product lifetime;old product;product generation;new technology;environmental benefit;environmental choice;environmental consequence'), (1, '')]\n"
     ]
    }
   ],
   "source": [
    "print(authorItemTuples.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2e0e5de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "[Stage 73:====================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1712433, {'authors_data': {'author_id': '1712433', 'name': 'Andrea Gantchev', 'paper_count': '2', 'citation_count': '3', 'h_index': '1'}, 'research_interests_data': {'author_id': '1712433', 'research_interests': 'subsumption architecture;Subsumption ArchitectureThe subsumption architecture;software architecture;subsumption architectureReusable Strategies;Object-oriented design;object-oriented software design;Rodney Brooks;Software Agents;behaviour-based control;different micro-strategies'}})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "authorDictsArray = convertDataIntoDicts(authorItemTuples, AUTHOR_KEYS_MAP, reduceAuthorFeaturesToDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5da9d7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'authors_data': {'author_id': '17', 'name': 'J. Michael Howe', 'paper_count': '1', 'citation_count': '0', 'h_index': '0'}, 'research_interests_data': {'author_id': '17', 'research_interests': 'HIV disease;Internet resource;World-Wide Web;clinical management'}}, {'authors_data': {'author_id': '34', 'name': 'Haitham Gabr', 'paper_count': '2', 'citation_count': '0', 'h_index': '0'}, 'research_interests_data': {'author_id': '34', 'research_interests': 'associate polynomial term;bivariate polynomial;difficult computational problem;novel polynomial;polynomial multiplication problem;polynomial term;reachability problem;Probabilistic Reachability;Reachability analysis;better time complexity'}}, {'authors_data': {'author_id': '51', 'name': 'Emma Tonkin', 'paper_count': '8', 'citation_count': '4', 'h_index': '1'}, 'research_interests_data': {'author_id': '51', 'research_interests': 'metadata element;metadata record;Dublin Core metadata;PaperBase extracts metadata;Semi automated metadata extraction;applicationAs metadata providers increase;automated metadata extraction tool;metadata correction process;metadata revision process;metadata revision processMetRe'}}, {'authors_data': {'author_id': '68', 'name': 'Woochul Shin', 'paper_count': '4', 'citation_count': '1', 'h_index': '1'}, 'research_interests_data': {'author_id': '68', 'research_interests': 'Web Service;context information;mobile device;Web Service standard;Web technology;Web service operation;location-based GIS Web Service;location-based secured GIS Web;secured GIS Web Service;heterogeneous mobile device'}}, {'authors_data': {'author_id': '85', 'name': 'S Improta', 'paper_count': '1', 'citation_count': '0', 'h_index': '0'}, 'research_interests_data': {'author_id': '85', 'research_interests': 'intermediate key;key layering;key rotation;system key;encryption algorithm;encryption organization;authentication procedure;message authentication;message digit substitution;telesurveillance system'}}, {'authors_data': {'author_id': '102', 'name': 'Richard Ferri', 'paper_count': '5', 'citation_count': '8', 'h_index': '2'}, 'research_interests_data': {'author_id': '102', 'research_interests': 'feedback loop;different reuse measure;reuse growth factor;software reuse;reusable code repository;reusable library component;reusable software;software development process;Heterogenous Linux;LUI6Remote Linux explainedLearn'}}, {'authors_data': {'author_id': '119', 'name': 'Qing Liu', 'paper_count': '1', 'citation_count': '0', 'h_index': '0'}, 'research_interests_data': {'author_id': '119', 'research_interests': 'Rough Set;nomal Counter Propagation Neural;CP Neural Network;Counter Propagation Neural Network;Rough Membership Function;Rough Set theory;edge detection;Image Edge;good result'}}, {'authors_data': {'author_id': '136', 'name': 'Artur Gramacki', 'paper_count': '2', 'citation_count': '0', 'h_index': '0'}, 'research_interests_data': {'author_id': '136', 'research_interests': 'MATLAB toolbox;linear system;linear systemsDevelopment'}}, {'authors_data': {'author_id': '153', 'name': 'Olumuyiwa Oluwasanmi', 'paper_count': '2', 'citation_count': '0', 'h_index': '0'}, 'research_interests_data': {'author_id': '153', 'research_interests': 'Byzantine agreement;algorithms work;random bit;reliable algorithm;Random Beacon;Byzantine agreement problem;scalable byzantine agreement;classical problem;important problem;serious problem'}}, {'authors_data': {'author_id': '170', 'name': 'Josef Willenborg', 'paper_count': '1', 'citation_count': '0', 'h_index': '0'}, 'research_interests_data': {'author_id': '170', 'research_interests': 'Ein objektorientiertes System zur;tzung der Thesauruspflege'}}]\n"
     ]
    }
   ],
   "source": [
    "print(authorDictsArray.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bed74803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve research interests and authors data\n",
    "authors_d = authorDictsArray.map(lambda x: x[\"authors_data\"])\n",
    "research_interests_d = authorDictsArray.map(lambda x: x[\"research_interests_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc3f39af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------------------+-----------+\n",
      "|author_id|h_index|                name|paper_count|\n",
      "+---------+-------+--------------------+-----------+\n",
      "|       17|      0|     J. Michael Howe|          1|\n",
      "|       34|      0|        Haitham Gabr|          2|\n",
      "|       51|      1|         Emma Tonkin|          8|\n",
      "|       68|      1|        Woochul Shin|          4|\n",
      "|       85|      0|           S Improta|          1|\n",
      "|      102|      2|       Richard Ferri|          5|\n",
      "|      119|      0|            Qing Liu|          1|\n",
      "|      136|      0|      Artur Gramacki|          2|\n",
      "|      153|      0|Olumuyiwa Oluwasanmi|          2|\n",
      "|      170|      0|    Josef Willenborg|          1|\n",
      "|      187|      0|            Qing Wei|          1|\n",
      "|      204|      1|Jurey Ivanovich Z...|          1|\n",
      "|      221|      1|             Anny Ng|          1|\n",
      "|      238|      1|    Nikos B. Pronios|          3|\n",
      "|      255|      0| Lourdes Fraga Alman|          1|\n",
      "|      272|      1|       Junji Nishino|          7|\n",
      "|      289|      0|       L. Dascalescu|          2|\n",
      "|      306|      0|    Masakazu Nishino|          1|\n",
      "|      323|      1|    Jean-RÃ©mi Duquet|          2|\n",
      "|      340|      1|     Brian A. Canada|          2|\n",
      "+---------+-------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|author_id|  research_interests|\n",
      "+---------+--------------------+\n",
      "|       17|HIV disease;Inter...|\n",
      "|       34|associate polynom...|\n",
      "|       51|metadata element;...|\n",
      "|       68|Web Service;conte...|\n",
      "|       85|intermediate key;...|\n",
      "|      102|feedback loop;dif...|\n",
      "|      119|Rough Set;nomal C...|\n",
      "|      136|MATLAB toolbox;li...|\n",
      "|      153|Byzantine agreeme...|\n",
      "|      170|Ein objektorienti...|\n",
      "|      187|portable device;A...|\n",
      "|      204|Integer-valued pr...|\n",
      "|      221|stock price;stock...|\n",
      "|      238|Hypermedia Synchr...|\n",
      "|      255|computer-mediated...|\n",
      "|      272|Dijkstra method;o...|\n",
      "|      289|low-frequency act...|\n",
      "|      306|copyright process...|\n",
      "|      323|uncertain informa...|\n",
      "|      340|histology image;s...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "convertDictsArrayIntoCSVFile(authors_d, \"authors\", PATH_TO_ASSETS)\n",
    "convertDictsArrayIntoCSVFile(research_interests_d, \"research_interests\", PATH_TO_ASSETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db785c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieve the files into a correct folder\n",
    "moveFileToCorrectFolder('authors', PATH_TO_ASSETS)\n",
    "moveFileToCorrectFolder('research_interests', PATH_TO_ASSETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ff9e74",
   "metadata": {},
   "source": [
    "## Convert the author-id-2-paper-id dataset using above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f21d021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "      .add(\"index\",IntegerType(),True) \\\n",
    "      .add(\"author_id\",IntegerType(),True) \\\n",
    "      .add(\"paper_id\",IntegerType(),True) \\\n",
    "      .add(\"author_position\",StringType(),True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22182237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# readFileA2P = spark.read.options(delimiter='\\t').csv(AUTHOR_2_PAPER_DATASET_PATH, header=False,schema=schema)\n",
    "\n",
    "author_id_2_paper_id_df = createDFFromFileAndSchema( \\\n",
    "    spark, \\\n",
    "    AUTHOR_2_PAPER_DATASET_PATH, \\\n",
    "    AUTHOR_2_PAPER_SCHEMA_PATH, \\\n",
    "    '\\t', \\\n",
    "    False \\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4704954d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- author_id: integer (nullable = true)\n",
      " |-- paper_id: integer (nullable = true)\n",
      " |-- author_position: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "author_id_2_paper_id_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d49d739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+--------+---------------+\n",
      "|index|author_id|paper_id|author_position|\n",
      "+-----+---------+--------+---------------+\n",
      "|    1|   381617|       1|              1|\n",
      "|    2|   630546|       3|              1|\n",
      "|    3|   112127|       4|              1|\n",
      "|    4|    96116|       4|              2|\n",
      "|    5|   578328|       5|              1|\n",
      "|    6|   865779|       5|              2|\n",
      "|    7|   669143|       5|              3|\n",
      "|    8|   533344|       6|              1|\n",
      "|    9|   621167|       7|              1|\n",
      "|   10|   522333|       7|              2|\n",
      "|   11|   597188|       7|              3|\n",
      "|   12|  1396373|       8|              1|\n",
      "|   13|  1644597|       8|              2|\n",
      "|   14|   798283|       8|              3|\n",
      "|   15|   371951|       9|              1|\n",
      "|   16|   378500|      10|              1|\n",
      "|   17|   117256|      11|              1|\n",
      "|   18|   562284|      12|              1|\n",
      "|   19|     1224|      13|              1|\n",
      "|   20|  1223056|      14|              1|\n",
      "+-----+---------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "author_id_2_paper_id_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4fe116cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/20 11:00:59 WARN TransportChannelHandler: Exception in connection from /192.168.0.101:62797\n",
      "java.io.IOException: Operation timed out\n",
      "\tat java.base/sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\tat java.base/sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:276)\n",
      "\tat java.base/sun.nio.ch.IOUtil.read(IOUtil.java:233)\n",
      "\tat java.base/sun.nio.ch.IOUtil.read(IOUtil.java:223)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:356)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:253)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "### Change the delimiter to comma, add headers and save the csv\n",
    "saveDFIntoCSVFoldersaveDFIntoCSVFolder(author_id_2_paper_id_df, 'paper_author_id', PATH_TO_ASSETS)\n",
    "moveFileToCorrectFolder('paper_author_id', PATH_TO_ASSETS)\n",
    "\n",
    "# readFileA2P.coalesce(1).write.option(\"header\",True).option(\"delimiter\",\",\").csv(\"paper-author.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
