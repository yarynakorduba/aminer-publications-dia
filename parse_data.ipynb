{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e096de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pandas as pd;\n",
    "import csv;\n",
    "import numpy as np;\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "600b2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('Read & parse text data file in pyspark')\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc32d019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile('./assets/AMiner-Paper.txt').zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5811092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find the starting row of each data entry\n",
    "pos = lines.filter(lambda x: \"#index\" in x[0]).map(lambda x: x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c71dc380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "zeroArray = np.zeros(lines.count(), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f92fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exchangeElementAtIndexToOne(index):\n",
    "    zeroArray[index] = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4c2f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in pos:\n",
    "    exchangeElementAtIndexToOne(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ecbf478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative sum of starting data entry indexes\n",
    "summedArray = np.cumsum(zeroArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63589899",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastedArray = sc.broadcast(summedArray);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75b0a8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBroarcastedTuple(x):\n",
    "    return (broadcastedArray.value[x[1]], x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf5aedaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = lines.map(lambda x: createBroarcastedTuple(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aacf1ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_get = lambda l, x, d=None: d if not l[x] else l[x] # safe getter of list values\n",
    "\n",
    "paper_keys = [\"#index\", \"#*\", \"#@\", \"#o\", \"#t\", \"#c\", \"#%\", \"#!\"]\n",
    "\n",
    "# author_keys = [\"#index\", \"#n\", \"#a\", \"#pc\", \"#cn\", \"#hi\", \"#pi\", \"#upi\", \"#t\"]\n",
    "# AUTHOR_KEYS_MAP = {\n",
    "#     \"#index\": \"id\", \"#n\": \"name\", \"#a\": \"affiliations\", \"#pc\": \"paper_count\", \"#cn\": \"citation_count\", \"#t\": \"research_interests\", \"#hi\": \"h_index\", \"#upi\": \"u_p_index\", \"#pi\": \"p_index\"\n",
    "# }\n",
    "\n",
    "PAPER_KEYS_MAP = {\n",
    "    \"#index\": \"id\", \"#*\": \"title\", \"#@\": \"authors\", \"#o\": \"affiliations\", \"#t\": \"year\", \"#c\": \"publication_venue\", \"#%\": \"ref_ids\", \"#!\": \"abstract\",\n",
    "}\n",
    "\n",
    "with open('./assets/parsedData/authors.csv', 'w', encoding='utf8', newline='') as output_file:\n",
    "    fc = csv.DictWriter(output_file, fieldnames=paper_keys)\n",
    "    fc.writeheader()\n",
    "#     fc.writerows(mappedPaperDicts.collect())\n",
    "\n",
    "def splitIntoKeyValue(stringToSplit):\n",
    "    formattedValue = stringToSplit \n",
    "    if type(stringToSplit) is str:\n",
    "        if len(formattedValue) > 0:\n",
    "            formattedValue = formattedValue.split(\" \", 1)\n",
    "            key = list_get(formattedValue, 0)\n",
    "            mappedKey = PAPER_KEYS_MAP.get(key) # note : this is not ideal. We should check and write better\n",
    "#             print(\">>> \", mappedKey)\n",
    "            if (mappedKey):\n",
    "                formattedValue = { mappedKey: list_get(formattedValue, 1, '') }\n",
    "            else:\n",
    "                print(\"-> \", key, stringToSplit)\n",
    "                formattedValue = {}\n",
    "        else:\n",
    "            formattedValue = {}\n",
    "    return formattedValue\n",
    "\n",
    "def reduceFeaturesToDict(featureA='', featureB=''):\n",
    "#     print(\"here\")\n",
    "    try:        \n",
    "        splittedA = splitIntoKeyValue(featureA);\n",
    "        splittedB = splitIntoKeyValue(featureB);\n",
    "#         print(\"->>>>-->\",splittedA)\n",
    "#         if type(splittedA) is str:\n",
    "#             if len(splittedA) > 0:\n",
    "#                 splittedA = splittedA.split(\" \", 1)\n",
    "#                 mappedKey = AUTHOR_KEYS_MAP[list_get(splittedA, 0)]\n",
    "#                 if (mappedKey):\n",
    "#                     splittedA = { mappedKey: list_get(splittedA, 1, '') }\n",
    "#                 else:\n",
    "#                     splittedA = {}\n",
    "#             else:\n",
    "#                 splittedA = {}\n",
    "\n",
    "#         splittedB = featureB;\n",
    "#         if type(featureB) is str:\n",
    "#             if len(featureB) > 0:\n",
    "#                 splittedB = featureB.split(\" \", 1)\n",
    "#                 splittedB = { list_get(splittedB, 0): list_get(splittedB, 1, '') }\n",
    "#             else:\n",
    "#                 splittedB = {}\n",
    "#         if \n",
    "        return { **splittedA, **splittedB }\n",
    "    except:\n",
    "        print(\"ERROR: \", featureA, featureB)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "62f4c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedPapers = papers.reduceByKey(lambda a, b: reduceFeaturesToDict(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d84c871d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "[Stage 41:=====================================================>  (62 + 3) / 65]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2092356, {'id': '2092356', 'title': 'Reliability prediction through system modeling', 'authors': 'Lalit Kumar Singh;Gopika Vinod;A. K. Tripathi', 'affiliations': 'Dept of Computer Engg IIT(BHU) Varanasi, India;Reactor Safety Division Bhabha Atomic Research Centre Dept of Atomic Energy, Govt of India;Dept of Computer Engg IIT(BHU) Varanasi, India', 'year': '2013', 'publication_venue': 'ACM SIGSOFT Software Engineering Notes', 'ref_ids': '1056157', 'abstract': 'Quantifying software reliability, such as performance and dependability, through stochastic behavior models (or labeled transition systems) is already a common practice in the software analysis community. However, those models are usually too fine grained to represent an accurate view of the software system by its stakeholders. Scenarios, on the other hand, are capable not only to describe the system traces as behavior models do but also depict very clearly the system components designed to provide the intended system behavior as well as to outline a high level architecture view of the system being described. In this paper, we introduce a case study of a safety critical computer based system that is running in an Indian Nuclear Power Plant. We define clear component interfaces, from which we analyze its software reliability.'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(reducedPapers.sortByKey(ascending=False).first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "615a9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappedPaperDicts = reducedPapers.map(lambda x: x[1]) # retrieve dicts from the tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "946bedef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(mappedPaperDicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5cbac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3426830f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save data to csv file\n",
    "df.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"papers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7873861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappedPaperDicts.saveAsTextFile('./assets/parsedData/authors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcac749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
