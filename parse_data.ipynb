{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e096de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pandas as pd;\n",
    "import csv;\n",
    "import numpy as np;\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "import re\n",
    "from pyspark.sql.types import StringType, ArrayType,StructType,StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "600b2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('Read & parse text data file in pyspark')\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sqlContext = SQLContext.getOrCreate(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e0ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_KEYS_MAP = {\n",
    "    \"#index\": \"paper_id\",\n",
    "    \"#*\": \"title\",\n",
    "    \"#@\": \"authors\",\n",
    "    \"#o\": \"affiliations\",\n",
    "    \"#t\": \"year\",\n",
    "    \"#c\": \"publication_venue\",\n",
    "    \"#%\": \"ref_ids\",\n",
    "    \"#!\": \"abstract\",\n",
    "}\n",
    "PAPER_DATASET_PATH = './assets/AMiner-Paper.txt'\n",
    "\n",
    "AUTHOR_KEYS_MAP = {\n",
    "    \"#index\": \"author_id\",\n",
    "    \"#n\": \"name\",\n",
    "    \"#pc\": \"paper_count\",\n",
    "    \"#cn\": \"citation_count\",\n",
    "    \"#t\": \"research_interests\",\n",
    "    \"#hi\": \"h_index\", \n",
    "#     \"#upi\": \"u_p_index\", \n",
    "#     \"#pi\": \"p_index\"\n",
    "#     \"#a\": \"affiliations\",\n",
    "}\n",
    "AUTHOR_DATASET_PATH = './assets/AMiner-Author.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2ea5547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipDatasetWithIndex(datasetPath):\n",
    "    lines = sc.textFile(datasetPath).zipWithIndex()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b305f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exchangeElementAtIndexToOne(index, zeroArray):\n",
    "    zeroArray[index] = 1;\n",
    "\n",
    "def doCumSumForIndexRows(lines):\n",
    "    # Find the starting row of each data entry\n",
    "    pos = lines.filter(lambda x: \"#index\" in x[0]).map(lambda x: x[1]).collect() \n",
    "    zeroArray = np.zeros(lines.count(), dtype=int)\n",
    "    for element in pos:\n",
    "        exchangeElementAtIndexToOne(element, zeroArray)\n",
    "    # Calculate cumulative sum of starting data entry indexes\n",
    "    summedArray = np.cumsum(zeroArray)\n",
    "    return summedArray;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63589899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBroarcastedTuple(x, arrayOfStartRowIndexes):\n",
    "    return (arrayOfStartRowIndexes.value[x[1]], x[0])\n",
    "\n",
    "def convertDataIntoIndexedTuples(datasetLines, cumSummedIndexRowsArray):\n",
    "    broadcastedArray = sc.broadcast(cumSummedIndexRowsArray);\n",
    "    convertedData = datasetLines.map(lambda dataLine: createBroarcastedTuple(dataLine, broadcastedArray))\n",
    "    return convertedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33beefa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_get = lambda l, x, d=None: d if not l[x] else l[x] # safe getter of list values\n",
    "\n",
    "def splitIntoKeyValue(stringToSplit, keyMap):\n",
    "    formattedValue = stringToSplit \n",
    "    if type(stringToSplit) is str:\n",
    "        if len(formattedValue) > 0:\n",
    "            formattedValue = formattedValue.split(\" \", 1)\n",
    "            key = list_get(formattedValue, 0)\n",
    "            mappedKey = keyMap.get(key)\n",
    "            if (mappedKey):\n",
    "                formattedValue = { mappedKey: list_get(formattedValue, 1, '') }\n",
    "            else:\n",
    "                formattedValue = {}\n",
    "        else:\n",
    "            formattedValue = {}\n",
    "    return formattedValue\n",
    "\n",
    "def appendStringOrListIntoList(lst, elToAppend):\n",
    "    if elToAppend is not None:\n",
    "        if type(elToAppend) == str:\n",
    "            lst.append(elToAppend)\n",
    "        else:\n",
    "            lst = lst + elToAppend\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7623d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper related\n",
    "def convertPaperFeatures(dct, affiliations_data={}, papers_data={}, paper_authors_data={}, publication_venues_data={}, refs_data={}):\n",
    "    if dct.get(\"paper_id\"):\n",
    "        paper_id = dct.get(\"paper_id\")   \n",
    "        affiliations_data[\"paper_id\"] = paper_id\n",
    "        papers_data[\"paper_id\"] = paper_id\n",
    "        paper_authors_data[\"paper_id\"] = paper_id\n",
    "        publication_venues_data[\"paper_id\"] = paper_id\n",
    "        refs_data[\"paper_id\"] = paper_id\n",
    "    elif dct.get(\"affiliations\"):\n",
    "        affiliations_data[\"affiliations\"] = dct.get(\"affiliations\")\n",
    "    elif dct.get(\"ref_ids\"):\n",
    "        appendStringOrListIntoList(refs_data[\"ref_ids\"], dct.get(\"ref_ids\"))\n",
    "    elif dct.get(\"authors\"):\n",
    "        paper_authors_data[\"authors\"] = dct.get(\"authors\")\n",
    "    elif dct.get(\"title\"):\n",
    "        papers_data[\"title\"] = dct.get(\"title\")\n",
    "    elif dct.get(\"year\"):\n",
    "        papers_data[\"year\"] = dct.get(\"year\")\n",
    "    elif dct.get(\"publication_venue\"):\n",
    "        publication_venues_data[\"publication_venue\"] = dct.get(\"publication_venue\")\n",
    "        \n",
    "# Paper related\n",
    "def reducePaperFeaturesToDict(featureA='', featureB='', keyMap={}):\n",
    "    papers_data = {}\n",
    "    affiliations_data = {}\n",
    "    paper_authors_data = {}\n",
    "    publication_venues_data = {}\n",
    "    refs_data = { \"ref_ids\": [] }\n",
    "    try:        \n",
    "        splittedA = splitIntoKeyValue(featureA, keyMap);\n",
    "        splittedB = splitIntoKeyValue(featureB, keyMap);\n",
    "        \n",
    "        if (splittedA.get('papers_data')):\n",
    "            papers_data = splittedA.get('papers_data')\n",
    "            affiliations_data = splittedA.get('affiliations_data')\n",
    "            paper_authors_data = splittedA.get('paper_authors_data')\n",
    "            publication_venues_data = splittedA.get('publication_venues_data')\n",
    "            refs_data = splittedA.get('refs_data')\n",
    "        else:\n",
    "            convertPaperFeatures(splittedA, affiliations_data, papers_data, paper_authors_data, publication_venues_data, refs_data)\n",
    "        convertPaperFeatures(splittedB, affiliations_data, papers_data, paper_authors_data, publication_venues_data, refs_data)\n",
    "        return {\n",
    "            \"papers_data\": papers_data,\n",
    "            \"affiliations_data\": affiliations_data,\n",
    "            \"paper_authors_data\": paper_authors_data,\n",
    "            \"publication_venues_data\": publication_venues_data,\n",
    "            \"refs_data\": refs_data\n",
    "        }\n",
    "    except Exception as error:\n",
    "        print(\"ERROR: \", featureA, featureB, error)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aacf1ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author related\n",
    "def convertAuthorFeatures(dct, authors_data={}, research_interests_data={}):\n",
    "    if dct.get(\"author_id\"):\n",
    "        author_id = dct.get(\"author_id\")   \n",
    "        authors_data[\"author_id\"] = author_id\n",
    "        research_interests_data[\"author_id\"] = author_id\n",
    "    elif dct.get(\"name\"):\n",
    "        authors_data[\"name\"] = dct.get(\"name\")\n",
    "    elif dct.get(\"paper_count\"):\n",
    "        authors_data[\"paper_count\"] = dct.get(\"paper_count\")\n",
    "    elif dct.get(\"сitation_count\"):\n",
    "        authors_data[\"сitation_count\"] = dct.get(\"сitation_count\")\n",
    "    elif dct.get(\"research_interests\"):\n",
    "        research_interests_data[\"research_interests\"] = dct.get(\"research_interests\")\n",
    "    elif dct.get(\"h_index\"):\n",
    "        authors_data[\"h_index\"] = dct.get(\"h_index\")\n",
    "    elif dct.get(\"publication_venue\"):\n",
    "        authors_data[\"publication_venue\"] = dct.get(\"publication_venue\")\n",
    "        \n",
    "# Author related\n",
    "def reduceAuthorFeaturesToDict(featureA='', featureB='', keyMap={}):\n",
    "    authors_data = {}\n",
    "    research_interests_data = {}\n",
    "    try:        \n",
    "        splittedA = splitIntoKeyValue(featureA, keyMap);\n",
    "        splittedB = splitIntoKeyValue(featureB, keyMap);\n",
    "        \n",
    "        if (splittedA.get('authors_data')):\n",
    "            authors_data = splittedA.get('authors_data')\n",
    "            research_interests_data = splittedA.get('research_interests_data')\n",
    "        else: # If it's first reduce run, accumulator === the first item of the list. So the item should be converted\n",
    "            convertAuthorFeatures(splittedA, authors_data, research_interests_data)\n",
    "        convertAuthorFeatures(splittedB, authors_data, research_interests_data)\n",
    "        return {\n",
    "            \"authors_data\": authors_data,\n",
    "            \"research_interests_data\": research_interests_data,\n",
    "        }\n",
    "    except Exception as error:\n",
    "        print(\"ERROR: \", featureA, featureB, error)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "615a9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDataIntoDicts(objects, dataKeyMap, reducer):\n",
    "    reducedObjects = objects.reduceByKey(lambda a, b: reducer(a, b, dataKeyMap))\n",
    "    print(reducedObjects.sortByKey(ascending=False).first())\n",
    "    mappedDicts = reducedObjects.map(lambda x: x[1]) # retrieve dicts from the tuples\n",
    "    return mappedDicts\n",
    "\n",
    "def convertDictsArrayIntoCSVFile(dictsArray, fileName):\n",
    "    df = sqlContext.createDataFrame(dictsArray)\n",
    "    print(df.show())\n",
    "    # Save data to csv file\n",
    "    df.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "946bedef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "paperTextLines = zipDatasetWithIndex(PAPER_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5811092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summedArrayOfPaperIndexRows = doCumSumForIndexRows(paperTextLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bfa835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperItemTuples = convertDataIntoIndexedTuples(paperTextLines, summedArrayOfPaperIndexRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfcac749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "[Stage 8:========================================================>(64 + 1) / 65]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2092356, {'papers_data': {'paper_id': '2092356', 'title': 'Reliability prediction through system modeling', 'year': '2013'}, 'affiliations_data': {'paper_id': '2092356', 'affiliations': 'Dept of Computer Engg IIT(BHU) Varanasi, India;Reactor Safety Division Bhabha Atomic Research Centre Dept of Atomic Energy, Govt of India;Dept of Computer Engg IIT(BHU) Varanasi, India'}, 'paper_authors_data': {'paper_id': '2092356', 'authors': 'Lalit Kumar Singh;Gopika Vinod;A. K. Tripathi'}, 'publication_venues_data': {'paper_id': '2092356', 'publication_venue': 'ACM SIGSOFT Software Engineering Notes'}, 'refs_data': {'ref_ids': ['215579', '333683', '511383', '594375', '641666', '763878', '966860', '1056157'], 'paper_id': '2092356'}})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "paperDictsArray = convertDataIntoDicts(paperItemTuples, PAPER_KEYS_MAP, reducePaperFeaturesToDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08815e8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mapPapersData(data):\n",
    "    refs_data = data[\"refs_data\"]\n",
    "    refs_data['ref_ids'] = ';'.join(refs_data['ref_ids'])\n",
    "    return refs_data\n",
    "\n",
    "papers_d = paperDictsArray.map(lambda x: x[\"papers_data\"])\n",
    "affiliations_d = paperDictsArray.map(lambda x: x[\"affiliations_data\"])\n",
    "paper_authors_d = paperDictsArray.map(lambda x: x[\"paper_authors_data\"])\n",
    "publication_venues_d = paperDictsArray.map(lambda x: x[\"publication_venues_data\"])\n",
    "paper_refs_d = paperDictsArray.map(lambda x:mapPapersData(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cf3c692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:=======================================================>(64 + 1) / 65]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2092356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(papers_d.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "347d9e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|paper_id|             ref_ids|\n",
      "+--------+--------------------+\n",
      "|      65|                    |\n",
      "|     130|                    |\n",
      "|     195|317424;317425;317573|\n",
      "|     260|                    |\n",
      "|     325|                    |\n",
      "|     390|                    |\n",
      "|     455|                    |\n",
      "|     520|       318368;323493|\n",
      "|     585|                    |\n",
      "|     650|                    |\n",
      "|     715|                    |\n",
      "|     780|318420;319233;319...|\n",
      "|     845|                    |\n",
      "|     910|                    |\n",
      "|     975|67604;318882;3718...|\n",
      "|    1040|                    |\n",
      "|    1105|289087;318014;318...|\n",
      "|    1170|                    |\n",
      "|    1235|                    |\n",
      "|    1300|                    |\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# convertDictsArrayIntoCSVFile(papers_d, \"./assets/papers_d9.csv\")\n",
    "# convertDictsArrayIntoCSVFile(affiliations_d, \"./assets/affiliations_d9.csv\")\n",
    "# convertDictsArrayIntoCSVFile(paper_authors_d, \"./assets/paper_authors_d9.csv\")\n",
    "# convertDictsArrayIntoCSVFile(publication_venues_d, \"./assets/publication_venues_d9.csv\")\n",
    "convertDictsArrayIntoCSVFile(paper_refs_d, \"./assets/paper_refs_d9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b4f2e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Authors\n",
    "authorTextLines = zipDatasetWithIndex(AUTHOR_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfd6a922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summedArrayOfAuthorIndexRows = doCumSumForIndexRows(authorTextLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a05b5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorItemTuples = convertDataIntoIndexedTuples(authorTextLines, summedArrayOfAuthorIndexRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffb1f0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, '#index 1'), (1, '#n O. Willum'), (1, '#a Res. Center for Microperipherik, Technische Univ. Berlin, Germany'), (1, '#pc 1'), (1, '#cn 0'), (1, '#hi 0'), (1, '#pi 0.0000'), (1, '#upi 0.0000'), (1, '#t new product;product group;active product;long product lifetime;old product;product generation;new technology;environmental benefit;environmental choice;environmental consequence'), (1, '')]\n"
     ]
    }
   ],
   "source": [
    "print(authorItemTuples.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e0e5de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "[Stage 19:====================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1712433, {'authors_data': {'author_id': '1712433', 'name': 'Andrea Gantchev', 'paper_count': '2', 'h_index': '1'}, 'research_interests_data': {'author_id': '1712433', 'research_interests': 'subsumption architecture;Subsumption ArchitectureThe subsumption architecture;software architecture;subsumption architectureReusable Strategies;Object-oriented design;object-oriented software design;Rodney Brooks;Software Agents;behaviour-based control;different micro-strategies'}})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "authorDictsArray = convertDataIntoDicts(authorItemTuples, AUTHOR_KEYS_MAP, reduceAuthorFeaturesToDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bed74803",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_d = authorDictsArray.map(lambda x: x[\"authors_data\"])\n",
    "research_interests_d = authorDictsArray.map(lambda x: x[\"research_interests_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc3f39af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------------------+-----------+\n",
      "|author_id|h_index|                name|paper_count|\n",
      "+---------+-------+--------------------+-----------+\n",
      "|       17|      0|     J. Michael Howe|          1|\n",
      "|       34|      0|        Haitham Gabr|          2|\n",
      "|       51|      1|         Emma Tonkin|          8|\n",
      "|       68|      1|        Woochul Shin|          4|\n",
      "|       85|      0|           S Improta|          1|\n",
      "|      102|      2|       Richard Ferri|          5|\n",
      "|      119|      0|            Qing Liu|          1|\n",
      "|      136|      0|      Artur Gramacki|          2|\n",
      "|      153|      0|Olumuyiwa Oluwasanmi|          2|\n",
      "|      170|      0|    Josef Willenborg|          1|\n",
      "|      187|      0|            Qing Wei|          1|\n",
      "|      204|      1|Jurey Ivanovich Z...|          1|\n",
      "|      221|      1|             Anny Ng|          1|\n",
      "|      238|      1|    Nikos B. Pronios|          3|\n",
      "|      255|      0| Lourdes Fraga Alman|          1|\n",
      "|      272|      1|       Junji Nishino|          7|\n",
      "|      289|      0|       L. Dascalescu|          2|\n",
      "|      306|      0|    Masakazu Nishino|          1|\n",
      "|      323|      1|    Jean-Rémi Duquet|          2|\n",
      "|      340|      1|     Brian A. Canada|          2|\n",
      "+---------+-------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|author_id|  research_interests|\n",
      "+---------+--------------------+\n",
      "|       17|HIV disease;Inter...|\n",
      "|       34|associate polynom...|\n",
      "|       51|metadata element;...|\n",
      "|       68|Web Service;conte...|\n",
      "|       85|intermediate key;...|\n",
      "|      102|feedback loop;dif...|\n",
      "|      119|Rough Set;nomal C...|\n",
      "|      136|MATLAB toolbox;li...|\n",
      "|      153|Byzantine agreeme...|\n",
      "|      170|Ein objektorienti...|\n",
      "|      187|portable device;A...|\n",
      "|      204|Integer-valued pr...|\n",
      "|      221|stock price;stock...|\n",
      "|      238|Hypermedia Synchr...|\n",
      "|      255|computer-mediated...|\n",
      "|      272|Dijkstra method;o...|\n",
      "|      289|low-frequency act...|\n",
      "|      306|copyright process...|\n",
      "|      323|uncertain informa...|\n",
      "|      340|histology image;s...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "convertDictsArrayIntoCSVFile(authors_d, \"./assets/authors_d8.csv\")\n",
    "convertDictsArrayIntoCSVFile(research_interests_d, \"./assets/research_interests_d8.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
