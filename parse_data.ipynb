{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf7a7327",
   "metadata": {},
   "source": [
    "# Data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cf98b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /**\n",
    "# *                   _oo0oo_\n",
    "# *                  o8888888o\n",
    "# *                  88\" . \"88\n",
    "# *                  (| -_- |)\n",
    "# *                  0\\  =  /0\n",
    "# *                ___/`---'\\___\n",
    "# *              .' \\\\|     |// '.\n",
    "# *             / \\\\|||  :  |||// \\\n",
    "# *            / _||||| -:- |||||- \\\n",
    "# *           |   | \\\\\\  -  /// |   |\n",
    "# *           | \\_|  ''\\---/''  |_/ |\n",
    "# *           \\  .-\\__  '-'  ___/-. /\n",
    "# *         ___'. .'  /--.--\\  `. .'___\n",
    "# *      .\"\" '<  `.___\\_<|>_/___.' >' \"\".\n",
    "# *     | | :  `- \\`.;`\\ _ /`;.`/ - ` : | |\n",
    "# *     \\  \\ `_.   \\_ __\\ /__ _/   .-` /  /\n",
    "# * =====`-.____`.___ \\_____/___.-`___.-'=====\n",
    "# *                   `=---='\n",
    "# *\n",
    "# *\n",
    "# * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# *\n",
    "# *   Buddha blesses your code to be bug free\n",
    "# */"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaf3384",
   "metadata": {},
   "source": [
    "This file is used to parse the initial data from [AMiner dataset](https://www.aminer.org/aminernetwork). With this implementation we parse the text files (using cumulative sum applied to file lines) and split them into a couple of csvs which will later be used to create the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9e096de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pandas as pd;\n",
    "import csv;\n",
    "import numpy as np;\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "import re\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from helpers import createDFFromFileAndSchema, saveDFIntoCSVFolder, moveFileToCorrectFolder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "600b2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('Read& parse text data file in pyspark')\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sqlContext = SQLContext.getOrCreate(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "18e0ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_ASSETS = './assets/parsedData/'\n",
    "\n",
    "PAPER_KEYS_MAP = {\n",
    "    \"#index\": \"paper_id\",\n",
    "    \"#*\": \"title\",\n",
    "    \"#@\": \"authors\",\n",
    "    \"#o\": \"affiliations\",\n",
    "    \"#t\": \"year\",\n",
    "    \"#c\": \"publication_venue\",\n",
    "    \"#%\": \"ref_ids\",\n",
    "    \"#!\": \"abstract\",\n",
    "}\n",
    "PAPER_DATASET_PATH = './assets/AMiner-Paper.txt'\n",
    "\n",
    "AUTHOR_KEYS_MAP = {\n",
    "    \"#index\": \"author_id\",\n",
    "    \"#n\": \"name\",\n",
    "    \"#pc\": \"paper_count\",\n",
    "    \"#cn\": \"citation_count\",\n",
    "    \"#t\": \"research_interests\",\n",
    "    \"#hi\": \"h_index\", \n",
    "}\n",
    "AUTHOR_DATASET_PATH = './assets/AMiner-Author.txt'\n",
    "\n",
    "AUTHOR_2_PAPER_DATASET_PATH = './assets/AMiner-Author2Paper.txt'\n",
    "AUTHOR_2_PAPER_SCHEMA_PATH = './schemas/paper_author_id.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e2ea5547",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the file and zip each line into a tuple together with its index\n",
    "def zipDatasetWithIndex(datasetPath):\n",
    "    lines = sc.textFile(datasetPath).zipWithIndex()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c96a362",
   "metadata": {},
   "source": [
    "### Here we apply cumsum to find the row indexes with the data of each data entry\n",
    "1. Zero array: Create an array of 0s with the length equal to the number of lines in the file\n",
    "2. Find the indexes of the lines starting with `#index`\n",
    "3. Exchange those indexes inside Zero array into 1s.\n",
    "4. Apply cumulative sum to this array to find out entry row indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b305f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exchangeElementAtIndexToOne(index, zeroArray):\n",
    "    zeroArray[index] = 1;\n",
    "\n",
    "def doCumSumForIndexRows(lines):\n",
    "    # Find the starting row of each data entry\n",
    "    pos = lines.filter(lambda x: \"#index\" in x[0]).map(lambda x: x[1]).collect() \n",
    "    zeroArray = np.zeros(lines.count(), dtype=int)\n",
    "    for element in pos:\n",
    "        exchangeElementAtIndexToOne(element, zeroArray)\n",
    "    # Calculate cumulative sum of starting data entry indexes\n",
    "    summedArray = np.cumsum(zeroArray)\n",
    "    return summedArray;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63589899",
   "metadata": {},
   "outputs": [],
   "source": [
    "### These functions are used to gather the text lines for each data entry\n",
    "def createBroarcastedTuple(x, arrayOfStartRowIndexes):\n",
    "    return (arrayOfStartRowIndexes.value[x[1]], x[0])\n",
    "\n",
    "def convertDataIntoIndexedTuples(datasetLines, cumSummedIndexRowsArray):\n",
    "    broadcastedArray = sc.broadcast(cumSummedIndexRowsArray);\n",
    "    convertedData = datasetLines.map(lambda dataLine: createBroarcastedTuple(dataLine, broadcastedArray))\n",
    "    return convertedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33beefa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function is used to split string into key and value\n",
    "list_get = lambda l, x, d=None: d if not l[x] else l[x] # safe getter of list values\n",
    "\n",
    "def splitIntoKeyValue(stringToSplit, keyMap):\n",
    "    formattedValue = stringToSplit \n",
    "    if type(stringToSplit) is str:\n",
    "        if len(formattedValue) > 0:\n",
    "            formattedValue = formattedValue.split(\" \", 1)\n",
    "            key = list_get(formattedValue, 0)\n",
    "            mappedKey = keyMap.get(key)\n",
    "            if (mappedKey):\n",
    "                formattedValue = { mappedKey: list_get(formattedValue, 1, '') }\n",
    "            else:\n",
    "                formattedValue = {}\n",
    "        else:\n",
    "            formattedValue = {}\n",
    "    return formattedValue\n",
    "\n",
    "def appendStringOrListIntoList(lst, elToAppend):\n",
    "    if elToAppend is not None:\n",
    "        if type(elToAppend) == str:\n",
    "            lst.append(elToAppend)\n",
    "        else:\n",
    "            lst = lst + elToAppend\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7623d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "### These functios are used to convert the strings with paper features for a data entry\n",
    "### into a dictionnary with mapped key and value\n",
    "\n",
    "### Paper related\n",
    "def convertPaperFeatures(dct, affiliations_data={}, papers_data={}, paper_authors_data={}, publication_venues_data={}, refs_data={}):\n",
    "    if dct.get(\"paper_id\"):\n",
    "        paper_id = dct.get(\"paper_id\")   \n",
    "        affiliations_data[\"paper_id\"] = paper_id\n",
    "        papers_data[\"paper_id\"] = paper_id\n",
    "        paper_authors_data[\"paper_id\"] = paper_id\n",
    "        publication_venues_data[\"paper_id\"] = paper_id\n",
    "        refs_data[\"paper_id\"] = paper_id\n",
    "    elif dct.get(\"affiliations\"):\n",
    "        affiliations_data[\"affiliations\"] = dct.get(\"affiliations\")\n",
    "    elif dct.get(\"ref_ids\"):\n",
    "        appendStringOrListIntoList(refs_data[\"ref_ids\"], dct.get(\"ref_ids\"))\n",
    "    elif dct.get(\"authors\"):\n",
    "        paper_authors_data[\"authors\"] = dct.get(\"authors\")\n",
    "    elif dct.get(\"title\"):\n",
    "        papers_data[\"title\"] = dct.get(\"title\")\n",
    "    elif dct.get(\"year\"):\n",
    "        papers_data[\"year\"] = dct.get(\"year\")\n",
    "    elif dct.get(\"publication_venue\"):\n",
    "        publication_venues_data[\"publication_venue\"] = dct.get(\"publication_venue\")\n",
    "        \n",
    "# Paper related\n",
    "def reducePaperFeaturesToDict(featureA='', featureB='', keyMap={}):\n",
    "    papers_data = {}\n",
    "    affiliations_data = {}\n",
    "    paper_authors_data = {}\n",
    "    publication_venues_data = {}\n",
    "    refs_data = { \"ref_ids\": [] }\n",
    "    try:        \n",
    "        splittedA = splitIntoKeyValue(featureA, keyMap);\n",
    "        splittedB = splitIntoKeyValue(featureB, keyMap);\n",
    "        \n",
    "        if (splittedA.get('papers_data')):\n",
    "            papers_data = splittedA.get('papers_data')\n",
    "            affiliations_data = splittedA.get('affiliations_data')\n",
    "            paper_authors_data = splittedA.get('paper_authors_data')\n",
    "            publication_venues_data = splittedA.get('publication_venues_data')\n",
    "            refs_data = splittedA.get('refs_data')\n",
    "        else:\n",
    "            convertPaperFeatures(splittedA, affiliations_data, papers_data, paper_authors_data, publication_venues_data, refs_data)\n",
    "        convertPaperFeatures(splittedB, affiliations_data, papers_data, paper_authors_data, publication_venues_data, refs_data)\n",
    "        return {\n",
    "            \"papers_data\": papers_data,\n",
    "            \"affiliations_data\": affiliations_data,\n",
    "            \"paper_authors_data\": paper_authors_data,\n",
    "            \"publication_venues_data\": publication_venues_data,\n",
    "            \"refs_data\": refs_data\n",
    "        }\n",
    "    except Exception as error:\n",
    "        print(\"ERROR: \", featureA, featureB, error)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aacf1ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### These functios are used to convert the strings with author features for a data entry\n",
    "### into a dictionnary with mapped key and value\n",
    "\n",
    "# Author related\n",
    "def convertAuthorFeatures(dct, authors_data={}, research_interests_data={}):\n",
    "    if dct.get(\"author_id\"):\n",
    "        author_id = dct.get(\"author_id\")   \n",
    "        authors_data[\"author_id\"] = author_id\n",
    "        research_interests_data[\"author_id\"] = author_id\n",
    "    elif dct.get(\"name\"):\n",
    "        authors_data[\"name\"] = dct.get(\"name\")\n",
    "    elif dct.get(\"paper_count\"):\n",
    "        authors_data[\"paper_count\"] = dct.get(\"paper_count\")\n",
    "    elif dct.get(\"citation_count\"):\n",
    "        authors_data[\"citation_count\"] = dct.get(\"citation_count\")\n",
    "    elif dct.get(\"research_interests\"):\n",
    "        research_interests_data[\"research_interests\"] = dct.get(\"research_interests\")\n",
    "    elif dct.get(\"h_index\"):\n",
    "        authors_data[\"h_index\"] = dct.get(\"h_index\")\n",
    "    elif dct.get(\"publication_venue\"):\n",
    "        authors_data[\"publication_venue\"] = dct.get(\"publication_venue\")\n",
    "        \n",
    "# Author related\n",
    "def reduceAuthorFeaturesToDict(featureA='', featureB='', keyMap={}):\n",
    "    authors_data = {}\n",
    "    research_interests_data = {}\n",
    "    try:        \n",
    "        splittedA = splitIntoKeyValue(featureA, keyMap);\n",
    "        splittedB = splitIntoKeyValue(featureB, keyMap);\n",
    "        \n",
    "        if (splittedA.get('authors_data')):\n",
    "            authors_data = splittedA.get('authors_data')\n",
    "            research_interests_data = splittedA.get('research_interests_data')\n",
    "        else: # If it's first reduce run, accumulator === the first item of the list. So the item should be converted\n",
    "            convertAuthorFeatures(splittedA, authors_data, research_interests_data)\n",
    "        convertAuthorFeatures(splittedB, authors_data, research_interests_data)\n",
    "        return {\n",
    "            \"authors_data\": authors_data,\n",
    "            \"research_interests_data\": research_interests_data,\n",
    "        }\n",
    "    except Exception as error:\n",
    "        print(\"ERROR: \", featureA, featureB, error)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "615a9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDataIntoDicts(objects, dataKeyMap, reducer):\n",
    "    reducedObjects = objects.reduceByKey(lambda a, b: reducer(a, b, dataKeyMap))\n",
    "    print(reducedObjects.sortByKey(ascending=False).first())\n",
    "    mappedDicts = reducedObjects.map(lambda x: x[1]) # retrieve dicts from the tuples\n",
    "    return mappedDicts\n",
    "\n",
    "def convertDictsArrayIntoCSVFile(dictsArray, folderName, pathToFolder):\n",
    "    df = sqlContext.createDataFrame(dictsArray)\n",
    "    print(df.show())\n",
    "    # Save data to csv file\n",
    "    df.coalesce(1).write \\\n",
    "        .format(\"com.databricks.spark.csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .save(f'{pathToFolder}{folderName}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e10124",
   "metadata": {},
   "source": [
    "## Convert the papers dataset using above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "946bedef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "paperTextLines = zipDatasetWithIndex(PAPER_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5811092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summedArrayOfPaperIndexRows = doCumSumForIndexRows(paperTextLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5bfa835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperItemTuples = convertDataIntoIndexedTuples(paperTextLines, summedArrayOfPaperIndexRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bfcac749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "[Stage 18:=======================================================>(64 + 1) / 65]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2092356, {'papers_data': {'paper_id': '2092356', 'title': 'Reliability prediction through system modeling', 'year': '2013'}, 'affiliations_data': {'paper_id': '2092356', 'affiliations': 'Dept of Computer Engg IIT(BHU) Varanasi, India;Reactor Safety Division Bhabha Atomic Research Centre Dept of Atomic Energy, Govt of India;Dept of Computer Engg IIT(BHU) Varanasi, India'}, 'paper_authors_data': {'paper_id': '2092356', 'authors': 'Lalit Kumar Singh;Gopika Vinod;A. K. Tripathi'}, 'publication_venues_data': {'paper_id': '2092356', 'publication_venue': 'ACM SIGSOFT Software Engineering Notes'}, 'refs_data': {'ref_ids': ['215579', '333683', '511383', '594375', '641666', '763878', '966860', '1056157'], 'paper_id': '2092356'}})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "paperDictsArray = convertDataIntoDicts(paperItemTuples, PAPER_KEYS_MAP, reducePaperFeaturesToDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "08815e8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mapPapersData(data):\n",
    "    refs_data = data[\"refs_data\"]\n",
    "    refs_data['ref_ids'] = ';'.join(refs_data['ref_ids'])\n",
    "    return refs_data\n",
    "\n",
    "papers_d = paperDictsArray.map(lambda x: x[\"papers_data\"])\n",
    "affiliations_d = paperDictsArray.map(lambda x: x[\"affiliations_data\"])\n",
    "paper_authors_d = paperDictsArray.map(lambda x: x[\"paper_authors_data\"])\n",
    "publication_venues_d = paperDictsArray.map(lambda x: x[\"publication_venues_data\"])\n",
    "paper_refs_d = paperDictsArray.map(lambda x:mapPapersData(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347d9e57",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----+\n",
      "|paper_id|               title|year|\n",
      "+--------+--------------------+----+\n",
      "|      65|Direct file organ...|1984|\n",
      "|     130|An introduction t...|1983|\n",
      "|     195|On solving almost...|1984|\n",
      "|     260|Connections betwe...|1984|\n",
      "|     325|Computers and pen...|1984|\n",
      "|     390|Relativizations c...|1984|\n",
      "|     455|On the optimum ch...|1984|\n",
      "|     520|All points addres...|1984|\n",
      "|     585|Optimum Head Sepa...|1984|\n",
      "|     650|A parallel-design...|1984|\n",
      "|     715|Computer - IEEE C...|1984|\n",
      "|     780|Experience with G...|1984|\n",
      "|     845|Code generation a...|1984|\n",
      "|     910|On estimating acc...|1984|\n",
      "|     975|A distributed alt...|1985|\n",
      "|    1040|A comparison of t...|1984|\n",
      "|    1105|Generalizing spec...|1985|\n",
      "|    1170|Real time graphic...|1984|\n",
      "|    1235|Common and uncomm...|1984|\n",
      "|    1300|Foundations of co...|1985|\n",
      "+--------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    }
   ],
   "source": [
    "convertDictsArrayIntoCSVFile(papers_d, \"papers\", PATH_TO_ASSETS)\n",
    "convertDictsArrayIntoCSVFile(affiliations_d, \"affiliations\", PATH_TO_ASSETS)\n",
    "convertDictsArrayIntoCSVFile(paper_authors_d, \"paper_authors\", PATH_TO_ASSETS)\n",
    "convertDictsArrayIntoCSVFile(publication_venues_d, \"publication_venues\", PATH_TO_ASSETS)\n",
    "convertDictsArrayIntoCSVFile(paper_refs_d, \"paper_refs\", PATH_TO_ASSETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef5c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieve the files into a correct folder\n",
    "moveFileToCorrectFolder('papers', PATH_TO_ASSETS)\n",
    "moveFileToCorrectFolder('affiliations', PATH_TO_ASSETS)\n",
    "moveFileToCorrectFolder('paper_authors', PATH_TO_ASSETS)\n",
    "moveFileToCorrectFolder('publication_venues', PATH_TO_ASSETS)\n",
    "moveFileToCorrectFolder('paper_refs', PATH_TO_ASSETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d998e",
   "metadata": {},
   "source": [
    "## Convert the authors dataset using above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b4f2e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Authors\n",
    "authorTextLines = zipDatasetWithIndex(AUTHOR_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bfd6a922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "summedArrayOfAuthorIndexRows = doCumSumForIndexRows(authorTextLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a05b5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorItemTuples = convertDataIntoIndexedTuples(authorTextLines, summedArrayOfAuthorIndexRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e0e5de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "/usr/local/Cellar/apache-spark/3.2.0/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1712433, {'authors_data': {'author_id': '1712433', 'name': 'Andrea Gantchev', 'paper_count': '2', 'citation_count': '3', 'h_index': '1'}, 'research_interests_data': {'author_id': '1712433', 'research_interests': 'subsumption architecture;Subsumption ArchitectureThe subsumption architecture;software architecture;subsumption architectureReusable Strategies;Object-oriented design;object-oriented software design;Rodney Brooks;Software Agents;behaviour-based control;different micro-strategies'}})\n"
     ]
    }
   ],
   "source": [
    "authorDictsArray = convertDataIntoDicts(authorItemTuples, AUTHOR_KEYS_MAP, reduceAuthorFeaturesToDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bed74803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve research interests and authors data\n",
    "authors_d = authorDictsArray.map(lambda x: x[\"authors_data\"])\n",
    "research_interests_d = authorDictsArray.map(lambda x: x[\"research_interests_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f39af",
   "metadata": {},
   "outputs": [],
   "source": [
    "convertDictsArrayIntoCSVFile(authors_d, \"authors\", PATH_TO_ASSETS)\n",
    "convertDictsArrayIntoCSVFile(research_interests_d, \"research_interests\", PATH_TO_ASSETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db785c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieve the files into a correct folder\n",
    "moveFileToCorrectFolder('authors', PATH_TO_ASSETS)\n",
    "moveFileToCorrectFolder('research_interests', PATH_TO_ASSETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ff9e74",
   "metadata": {},
   "source": [
    "## Convert the author-id-2-paper-id dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f21d021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "      .add(\"index\",IntegerType(),True) \\\n",
    "      .add(\"author_id\",IntegerType(),True) \\\n",
    "      .add(\"paper_id\",IntegerType(),True) \\\n",
    "      .add(\"author_position\",StringType(),True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "22182237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path: ./assets/AMiner-Author2Paper.txt, schema path: ./schemas/paper_author_id.csv\n",
      "Types from schema: [('index', 'Integer'), ('author_id', 'Integer'), ('paper_id', 'Integer'), ('author_position', 'Integer')]\n"
     ]
    }
   ],
   "source": [
    "# readFileA2P = spark.read.options(delimiter='\\t').csv(AUTHOR_2_PAPER_DATASET_PATH, header=False,schema=schema)\n",
    "\n",
    "author_id_2_paper_id_df = createDFFromFileAndSchema( \\\n",
    "    spark, \\\n",
    "    AUTHOR_2_PAPER_DATASET_PATH, \\\n",
    "    AUTHOR_2_PAPER_SCHEMA_PATH, \\\n",
    "    '\\t', \\\n",
    "    False \\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4704954d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- author_id: integer (nullable = true)\n",
      " |-- paper_id: integer (nullable = true)\n",
      " |-- author_position: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "author_id_2_paper_id_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1d49d739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 77:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+--------+---------------+\n",
      "|index|author_id|paper_id|author_position|\n",
      "+-----+---------+--------+---------------+\n",
      "|    1|   381617|       1|              1|\n",
      "|    2|   630546|       3|              1|\n",
      "|    3|   112127|       4|              1|\n",
      "|    4|    96116|       4|              2|\n",
      "|    5|   578328|       5|              1|\n",
      "|    6|   865779|       5|              2|\n",
      "|    7|   669143|       5|              3|\n",
      "|    8|   533344|       6|              1|\n",
      "|    9|   621167|       7|              1|\n",
      "|   10|   522333|       7|              2|\n",
      "|   11|   597188|       7|              3|\n",
      "|   12|  1396373|       8|              1|\n",
      "|   13|  1644597|       8|              2|\n",
      "|   14|   798283|       8|              3|\n",
      "|   15|   371951|       9|              1|\n",
      "|   16|   378500|      10|              1|\n",
      "|   17|   117256|      11|              1|\n",
      "|   18|   562284|      12|              1|\n",
      "|   19|     1224|      13|              1|\n",
      "|   20|  1223056|      14|              1|\n",
      "+-----+---------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "author_id_2_paper_id_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4fe116cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Change the delimiter to comma, add headers and save the csv\n",
    "saveDFIntoCSVFolder(author_id_2_paper_id_df, 'paper_author_id', PATH_TO_ASSETS)\n",
    "moveFileToCorrectFolder('paper_author_id', PATH_TO_ASSETS)\n",
    "\n",
    "# readFileA2P.coalesce(1).write.option(\"header\",True).option(\"delimiter\",\",\").csv(\"paper-author.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777ec6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
